{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chat models\n",
    "\n",
    "o3_mini = init_chat_model(\"openai:o3-mini\")\n",
    "claude_haiku = init_chat_model(\"anthropic:claude-3-haiku-20240307\", temperature=0)\n",
    "claude_sonnet = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0)\n",
    "phi = init_chat_model(\"ollama:phi:latest\", temperature=0)\n",
    "qwen1_5 = init_chat_model(\"ollama:qwen:0.5b\", temperature=0)\n",
    "vicuna = init_chat_model(\"ollama:vicuna:7b\", temperature=0)\n",
    "llama2 = init_chat_model(\"ollama:llama2:latest\", temperature=0)\n",
    "llama3_1 = init_chat_model(\"ollama:llama3.1:latest\", temperature=0)\n",
    "deepseek_r1 = init_chat_model(\"ollama:deepseek-r1:8b\", temperature=0)\n",
    "gemma3 = init_chat_model(\"ollama:gemma3:12b\", temperature=0)\n",
    "evaluator = init_chat_model(\"openai:gpt-4-turbo-2024-04-09\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing benchmark data and prompts, rubric\n",
    "\n",
    "bench_path = \"biggen_bench_test_4instance.json\"\n",
    "\n",
    "def load_benchmark_data(bench_path: str) -> List[Dict]:\n",
    "    \"\"\"Load benchmark data from a JSON file.\"\"\"\n",
    "    with open(bench_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "benchmark_data = load_benchmark_data(bench_path)\n",
    "\n",
    "def prepare_prompts(benchmark_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Prepare prompts by excluding reference_answer and score_rubric.\"\"\"\n",
    "    prompts = []\n",
    "    for item in benchmark_data:\n",
    "        prompt = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"capability\": item[\"capability\"],\n",
    "            \"task\": item[\"task\"],\n",
    "            \"instance_idx\": item[\"instance_idx\"],\n",
    "            \"system_prompt\": item[\"system_prompt\"],\n",
    "            \"input\": item[\"input\"],\n",
    "            # Exclude reference_answer and score_rubric\n",
    "        }\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "def prepare_rubric(benchmark_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Prepare rubric including reference_answer and score_rubric.\"\"\"\n",
    "    rubric = []\n",
    "    for item in benchmark_data:\n",
    "        prompt = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"reference_answer\": item[\"reference_answer\"],\n",
    "            \"score_rubric\": item[\"score_rubric\"]\n",
    "        }\n",
    "        rubric.append(prompt)\n",
    "    return rubric\n",
    "\n",
    "prompts = prepare_prompts(benchmark_data)\n",
    "rubrics = prepare_rubric(benchmark_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state structure\n",
    "class State(TypedDict):\n",
    "    prompts: List[Dict[str, Any]]\n",
    "    rubrics: Dict[str, Dict[str, Any]] \n",
    "#    processed_count: int\n",
    "    phi_results: List[Dict[str, Any]]\n",
    "    qwen1_5_results: List[Dict[str, Any]]\n",
    "    vicuna_results: List[Dict[str, Any]]\n",
    "    llama2_results: List[Dict[str, Any]]\n",
    "    llama3_1_results: List[Dict[str, Any]]\n",
    "    gemma3_results: List[Dict[str, Any]]\n",
    "#    deepseek_r1_results: List[Dict[str, Any]]\n",
    "    evaluation_results: List[Dict[str, Any]]\n",
    "    timestamp: str  # 워크플로우 전체에서 공유할 타임스탬프 필드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor_model function (node)\n",
    "\n",
    "def create_model_processor(model, model_name):\n",
    "    def process_model(state: State) -> Dict[str, Any]:\n",
    "        results = []\n",
    "        print(f\"--- Processing model: {model_name} ---\")\n",
    "        prompts_to_process = state.get(\"prompts\", [])\n",
    "        if not prompts_to_process:\n",
    "             print(f\"  No prompts found for {model_name}.\")\n",
    "             return {} # 처리할 프롬프트 없으면 빈 결과 반환\n",
    "\n",
    "        for prompt in prompts_to_process:\n",
    "            prompt_id = prompt.get('id', 'unknown_id') # ID 먼저 추출\n",
    "            system_prompt = prompt.get('system_prompt', '')\n",
    "            user_input = prompt.get('input', '')\n",
    "\n",
    "            if not user_input:\n",
    "                print(f\"  Skipping prompt {prompt_id} for {model_name} due to empty input.\")\n",
    "                result = {\n",
    "                    \"id\": prompt_id,\n",
    "                    \"model_name\": model_name,\n",
    "                    \"response\": None,\n",
    "                    \"error\": \"Skipped due to empty input\",\n",
    "                    \"latency\": 0\n",
    "                }\n",
    "                results.append(result)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                response = model.invoke(\n",
    "                    user_input,\n",
    "                    config={\"system_prompt\": system_prompt}\n",
    "                )\n",
    "                end_time = time.time()\n",
    "\n",
    "                # 응답 내용 추출 (다양한 응답 타입 처리)\n",
    "                response_content = None\n",
    "                if isinstance(response, BaseMessage) and hasattr(response, 'content'):\n",
    "                    response_content = response.content\n",
    "                elif isinstance(response, str):\n",
    "                    response_content = response\n",
    "                elif isinstance(response, dict) and 'content' in response: # Ollama 직접 호출 시\n",
    "                    response_content = response['content']\n",
    "                else:\n",
    "                    # 예상치 못한 응답 타입일 경우 문자열로 변환 시도\n",
    "                    try:\n",
    "                        response_content = str(response)\n",
    "                        print(f\"  Warning: Unexpected response type for {prompt_id} from {model_name}. Type: {type(response)}. Content extracted as string.\")\n",
    "                    except Exception as str_err:\n",
    "                         print(f\"  Error converting unexpected response type to string for {prompt_id} from {model_name}: {str_err}\")\n",
    "                         raise ValueError(f\"Unexpected response type and failed to convert to string: {type(response)}\")\n",
    "\n",
    "\n",
    "                latency = end_time - start_time\n",
    "\n",
    "                # 성공 시 결과 딕셔너리\n",
    "                result = {\n",
    "                    \"id\": prompt_id,\n",
    "                    \"model_name\": model_name,\n",
    "                    \"response\": response_content, # 추출된 응답 내용\n",
    "                    \"latency\": latency,\n",
    "                    \"error\": None # 성공 시 에러는 None\n",
    "                }\n",
    "                print(f\"  Successfully processed prompt {prompt_id} for {model_name} in {result['latency']:.2f}s\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # 오류 발생 시 상세 로깅 및 결과 딕셔너리\n",
    "                error_message = f\"Error processing prompt {prompt_id} for {model_name}: {type(e).__name__}: {e}\"\n",
    "                print(f\"  {error_message}\") # 콘솔에 상세 오류 출력\n",
    "\n",
    "                result = {\n",
    "                    \"id\": prompt_id,\n",
    "                    \"model_name\": model_name,\n",
    "                    \"response\": None, # 오류 시 response는 None\n",
    "                    \"error\": error_message, # 상세 오류 메시지 저장\n",
    "                    \"latency\": 0 # 오류 시 latency는 0\n",
    "                }\n",
    "            results.append(result)\n",
    "\n",
    "        results_key = f\"{model_name}_results\"\n",
    "        return {results_key: results} # 변경된 부분만 반환\n",
    "    return process_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processors\n",
    "process_phi = create_model_processor(phi, \"phi\")\n",
    "process_qwen1_5 = create_model_processor(qwen1_5, \"qwen1_5\")\n",
    "process_vicuna = create_model_processor(vicuna, \"vicuna\")\n",
    "process_llama2 = create_model_processor(llama2, \"llama2\")\n",
    "process_llama3_1 = create_model_processor(llama3_1, \"llama3_1\")\n",
    "process_gemma3 = create_model_processor(gemma3, \"gemma3\")\n",
    "# process_deepseek_r1 = create_model_processor(deepseek_r1, \"deepseek_r1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- define save intermediate results node ---\n",
    "def save_intermediate_results(state: State) -> Dict:\n",
    "    \"\"\"Saves the generated responses before evaluation using the timestamp from the state.\"\"\"\n",
    "    print(\"--- Saving Intermediate Results (Responses Only) ---\")\n",
    "    # State로부터 타임스탬프 가져오기\n",
    "    timestamp = state.get(\"timestamp\", datetime.now().strftime(\"%Y%m%d%H%M%S_fallback\"))\n",
    "    if not timestamp:\n",
    "        print(\"  Warning: Timestamp not found in state. Generating a new one for fallback.\")\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S_fallback\")\n",
    "    \n",
    "    output_dir = \"_output\"\n",
    "    os.makedirs(output_dir, exist_ok=True) # 출력 디렉토리 확인 및 생성\n",
    "\n",
    "    # 저장할 데이터 구성 (응답 결과만 선택)\n",
    "    data_to_save = {}\n",
    "    model_result_keys = [key for key in state if key.endswith(\"_results\") and key not in [\"evaluation_results\"]]\n",
    "    for key in model_result_keys:\n",
    "        if state.get(key):\n",
    "             data_to_save[key] = state[key]\n",
    "\n",
    "    # 프롬프트 정보도 함께 저장하고 싶다면 추가\n",
    "    # data_to_save[\"prompts\"] = state.get(\"prompts\", [])\n",
    "\n",
    "    if not data_to_save:\n",
    "        print(\"  No response data found to save.\")\n",
    "        return {} # 저장할 데이터 없으면 아무것도 안함\n",
    "\n",
    "    base_name, _ = os.path.splitext(os.path.basename(bench_path)) # 파일명 생성시 bench_path에서 확장자 제거\n",
    "    intermediate_filename = f\"{timestamp}_{base_name}_straight_responses_only.json\" # 수정됨\n",
    "    output_file_path = os.path.join(output_dir, intermediate_filename)\n",
    "\n",
    "    try:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data_to_save, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"  Intermediate results successfully saved to {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error saving intermediate results: {e}\")\n",
    "\n",
    "    # 이 노드는 상태를 변경하지 않으므로 빈 딕셔셔리 반환\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Evaluation node ---\n",
    "\n",
    "# 평가 프롬프트 템플릿 정의 (biggen_bench 구조 기반)\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert evaluator. Your task is to evaluate an AI assistant's response based on the provided user query, reference answer, and a detailed scoring rubric.\n",
    "Focus ONLY on the provided information and rubric. Assign a score from 1 to 5, where 5 is the best, according to the descriptions.\n",
    "Provide your output strictly in the specified format.\"\"\"),\n",
    "    (\"human\", \"\"\"\n",
    "**Evaluation Context:**\n",
    "\n",
    "* **Task Type:** {task_description}\n",
    "* **User Query:**\n",
    "    ```\n",
    "    {user_query}\n",
    "    ```\n",
    "* **Reference Answer:**\n",
    "    ```\n",
    "    {reference_answer}\n",
    "    ```\n",
    "* **AI Response to Evaluate:**\n",
    "    ```\n",
    "    {ai_response}\n",
    "    ```\n",
    "\n",
    "**Scoring Rubric:**\n",
    "\n",
    "* **Criteria:** {criteria}\n",
    "* **Score 1 Description:** {score1_desc}\n",
    "* **Score 2 Description:** {score2_desc}\n",
    "* **Score 3 Description:** {score3_desc}\n",
    "* **Score 4 Description:** {score4_desc}\n",
    "* **Score 5 Description:** {score5_desc}\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  Carefully compare the \"AI Response to Evaluate\" against the \"Reference Answer\" and the \"Scoring Rubric\".\n",
    "2.  Determine the score (1-5) that best reflects the quality of the AI Response according to the rubric descriptions.\n",
    "3.  Provide a brief rationale explaining *why* you chose that score, referencing specific aspects of the rubric descriptions and the AI response.\n",
    "\n",
    "**Output Format (MUST follow exactly):**\n",
    "Score: [Your score between 1-5]\n",
    "Rationale: [Your concise explanation based on the rubric]\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "# --- evaluate_responses 함수 내에서 이 템플릿을 사용하는 방법 ---\n",
    "\n",
    "def evaluate_responses(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluates responses from different models based on rubrics.\"\"\"\n",
    "    print(\"--- Starting Evaluation ---\")\n",
    "    all_evaluations = []\n",
    "    # State에서 benchmark_data 또는 그 매핑을 가져와야 함\n",
    "    # 예: benchmark_data가 evaluate_responses 스코프에서 사용 가능하다고 가정\n",
    "    benchmark_map_full = {item[\"id\"]: item for item in benchmark_data}\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "    evaluation_chain = evaluation_prompt_template | evaluator | parser\n",
    "\n",
    "    model_result_keys = [key for key in state if key.endswith(\"_results\") and key != \"evaluation_results\"]\n",
    "\n",
    "    for key in model_result_keys:\n",
    "        print(f\"  Evaluating results from: {key}\")\n",
    "        model_results = state.get(key, [])\n",
    "        for response_item in model_results:\n",
    "            prompt_id = response_item.get(\"id\")\n",
    "            model_name = response_item.get(\"model_name\")\n",
    "            response_content = response_item.get(\"response\")\n",
    "            error = response_item.get(\"error\")\n",
    "\n",
    "            if error:\n",
    "                eval_result = {\"id\": prompt_id, \"agent_name\": model_name, \"score\": None, \"rationale\": f\"Skipped due to error: {error}\", \"error\": True}\n",
    "                all_evaluations.append(eval_result)\n",
    "                continue\n",
    "\n",
    "            if not prompt_id or prompt_id not in benchmark_map_full:\n",
    "                print(f\"    Warning: Missing full benchmark data for prompt ID {prompt_id}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            benchmark_item = benchmark_map_full[prompt_id] # 해당 ID의 전체 benchmark 데이터\n",
    "\n",
    "            if not response_content:\n",
    "                 eval_result = {\"id\": prompt_id, \"agent_name\": model_name, \"score\": 0, \"rationale\": \"Empty response\", \"error\": False}\n",
    "                 all_evaluations.append(eval_result)\n",
    "                 continue\n",
    "\n",
    "            # --- 여기가 중요: input_data 딕셔너리 생성 ---\n",
    "            # benchmark_item (JSON 파일의 항목) 과 response_content (모델 응답)에서 값을 가져와\n",
    "            # evaluation_prompt_template 의 변수 이름에 매핑합니다.\n",
    "            input_data = {\n",
    "                \"task_description\": benchmark_item.get(\"task\", \"N/A\"),              # JSON의 'task' 필드\n",
    "                \"user_query\": benchmark_item.get(\"input\", \"N/A\"),                # JSON의 'input' 필드\n",
    "                \"reference_answer\": benchmark_item.get(\"reference_answer\", \"N/A\"),# JSON의 'reference_answer' 필드\n",
    "                \"ai_response\": response_content,                                 # LangGraph State에서 온 모델 응답\n",
    "                \"criteria\": benchmark_item.get(\"score_rubric\", {}).get(\"criteria\", \"N/A\"), # JSON의 'score_rubric'.'criteria'\n",
    "                \"score1_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score1_description\", \"N/A\"), # 이하 scoreX_description\n",
    "                \"score2_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score2_description\", \"N/A\"),\n",
    "                \"score3_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score3_description\", \"N/A\"),\n",
    "                \"score4_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score4_description\", \"N/A\"),\n",
    "                \"score5_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score5_description\", \"N/A\"),\n",
    "            }\n",
    "            # ---------------------------------------------\n",
    "\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                evaluation_output_str = evaluation_chain.invoke(input_data)\n",
    "                end_time = time.time()\n",
    "\n",
    "                # 출력 파싱 (이전 답변과 동일)\n",
    "                score = None\n",
    "                rationale = \"\"\n",
    "                score_match = re.search(r\"Score:\\s*(\\d)\", evaluation_output_str)\n",
    "                rationale_match = re.search(r\"Rationale:\\s*(.*)\", evaluation_output_str, re.DOTALL)\n",
    "\n",
    "                if score_match:\n",
    "                    score = int(score_match.group(1))\n",
    "                if rationale_match:\n",
    "                    rationale = rationale_match.group(1).strip()\n",
    "\n",
    "                if score is None or not rationale:\n",
    "                     print(f\"    Warning: Could not parse score/rationale for prompt {prompt_id}. Raw output: {evaluation_output_str}\")\n",
    "                     rationale = f\"Parsing Warning. Raw Output: {evaluation_output_str}\"\n",
    "\n",
    "                eval_result = {\n",
    "                    \"id\": prompt_id, \"agent_name\": model_name, \"score\": score,\n",
    "                    \"rationale\": rationale, \"latency\": end_time - start_time, \"error\": False\n",
    "                }\n",
    "                print(f\"    Evaluated prompt {prompt_id} from {model_name} in {eval_result['latency']:.2f}s. Score: {eval_result['score']}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Error evaluating prompt {prompt_id} from {model_name}: {e}\")\n",
    "                eval_result = {\n",
    "                    \"id\": prompt_id, \"agent_name\": model_name, \"score\": None,\n",
    "                    \"rationale\": f\"Evaluation failed: {str(e)}\", \"latency\": 0, \"error\": True\n",
    "                }\n",
    "            all_evaluations.append(eval_result)\n",
    "\n",
    "    print(\"--- Evaluation Finished ---\")\n",
    "    return {\"evaluation_results\": all_evaluations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- define workflow ---\n",
    "\n",
    "# 1. 워크플로우 시작 전에 타임스탬프 생성\n",
    "current_timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# 2. initial state 설정 시 생성된 타임스탬프 포함\n",
    "initial_state = {\n",
    "    \"prompts\": prompts,\n",
    "    \"rubrics\": rubrics,\n",
    "    # \"processed_count\": 0,\n",
    "    \"phi_results\": [],\n",
    "    \"qwen1_5_results\": [],\n",
    "    \"vicuna_results\": [],\n",
    "    \"llama2_results\": [],\n",
    "    \"llama3_1_results\": [],\n",
    "    \"gemma3_results\": [],\n",
    "    # \"deepseek_r1_results\": [],\n",
    "    \"evaluation_results\": [],\n",
    "    \"timestamp\": current_timestamp  # 생성된 타임스탬프를 State에 추가\n",
    "}\n",
    "\n",
    "# create workflow \n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# add nodes\n",
    "workflow.add_node(\"process_phi\", process_phi)\n",
    "workflow.add_node(\"process_qwen1_5\", process_qwen1_5)\n",
    "workflow.add_node(\"process_vicuna\", process_vicuna)\n",
    "workflow.add_node(\"process_llama2\", process_llama2)\n",
    "workflow.add_node(\"process_llama3\", process_llama3_1)\n",
    "workflow.add_node(\"process_gemma3\", process_gemma3)\n",
    "workflow.add_node(\"save_responses\", save_intermediate_results)\n",
    "workflow.add_node(\"evaluate\", evaluate_responses)\n",
    "\n",
    "# connect edges\n",
    "workflow.set_entry_point(\"process_phi\")\n",
    "workflow.add_edge(\"process_phi\", \"process_qwen1_5\")\n",
    "workflow.add_edge(\"process_qwen1_5\", \"process_vicuna\")\n",
    "workflow.add_edge(\"process_vicuna\", \"process_llama2\")\n",
    "workflow.add_edge(\"process_llama2\", \"process_llama3\")\n",
    "workflow.add_edge(\"process_llama3\", \"process_gemma3\")\n",
    "workflow.add_edge(\"process_gemma3\", \"save_responses\")\n",
    "workflow.add_edge(\"save_responses\", \"evaluate\")\n",
    "workflow.add_edge(\"evaluate\", END)\n",
    "\n",
    "# compile workflow \n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run workflow\n",
    "print(\"--- Starting Workflow ---\")\n",
    "final_state = app.invoke(initial_state)\n",
    "print(\"--- Workflow Finished ---\")\n",
    "\n",
    "# --- save final results (using timestamp from final_state) ---\n",
    "final_output_dir = \"_output\"\n",
    "os.makedirs(final_output_dir, exist_ok=True)\n",
    "\n",
    "# 3. 최종 상태에서 타임스탬프 가져와서 사용\n",
    "final_timestamp = final_state.get(\"timestamp\", datetime.now().strftime(\"%Y%m%d%H%M%S_final_fallback\"))\n",
    "if not final_timestamp:\n",
    "    print(\"  Warning: Timestamp not found in final state. Generating a new one for fallback.\")\n",
    "    final_timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S_final_fallback\")\n",
    "\n",
    "\n",
    "final_base_name, _ = os.path.splitext(os.path.basename(bench_path)) # 파일명 생성시 bench_path에서 확장자 제거\n",
    "final_filename = f\"{final_timestamp}_{final_base_name}_straight_with_evaluation.json\" # 수정됨\n",
    "final_output_file_path = os.path.join(final_output_dir, final_filename)\n",
    "\n",
    "try:\n",
    "    with open(final_output_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_state, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\nFinal results (with evaluation) successfully saved to {final_output_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving final results: {e}\")\n",
    "    print(f\"Final state type: {type(final_state)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import libraries\n",
    "# from IPython.display import display, Markdown, Image\n",
    "# import pydot # Mermaid 생성 시 내부적으로 필요할 수 있음\n",
    "# import graphviz # PNG 등 이미지 생성 시 시스템 설치 필요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- visualization workflow ---\n",
    "\n",
    "# print(\"--- LangGraph Workflow Visualization ---\")\n",
    "\n",
    "# # app 변수가 workflow.compile()을 통해 생성되었다고 가정\n",
    "# if 'app' in locals() and app is not None:\n",
    "#     try:\n",
    "#         # Mermaid 다이어그램 생성\n",
    "#         mermaid_string = app.get_graph().draw_mermaid()\n",
    "\n",
    "#         # Jupyter Notebook에서 Mermaid 렌더링\n",
    "#         print(\"Displaying Mermaid diagram:\")\n",
    "#         display(Markdown(f\"```mermaid\\n{mermaid_string}\\n```\"))\n",
    "\n",
    "#         # # (선택 사항) PNG 이미지 생성 및 표시\n",
    "#         # print(\"\\n--- LangGraph Workflow Structure (PNG) ---\")\n",
    "#         # try:\n",
    "#         #     output_filename = \"workflow_graph_test.png\"\n",
    "#         #     # PNG 파일로 직접 저장 시도\n",
    "#         #     app.get_graph().draw_png(path=output_filename)\n",
    "#         #     print(f\"Attempted to save PNG to {output_filename}. Check if the file exists in your notebook's directory.\")\n",
    "\n",
    "#         #     # 저장된 파일로부터 이미지 표시 시도 (선택 사항)\n",
    "#         #     from IPython.display import Image, display\n",
    "#         #     if os.path.exists(output_filename):\n",
    "#         #         display(Image(filename=output_filename))\n",
    "#         #     else:\n",
    "#         #         print(f\"File '{output_filename}' was not created.\")\n",
    "\n",
    "#         # except ImportError:\n",
    "#         #     print(\"PNG generation requires 'pydot' and 'graphviz' Python libraries.\")\n",
    "#         #     print(\"Also ensure Graphviz is installed on your system and added to PATH.\")\n",
    "#         #     print(\"(conda install python-graphviz pydot / pip install pydot graphviz)\")\n",
    "#         # except Exception as img_err:\n",
    "#         #     print(f\"Could not generate PNG: {img_err}\")\n",
    "#         #     print(\"Please check Graphviz installation.\")\n",
    "\n",
    "#     except Exception as viz_err:\n",
    "#         print(f\"An error occurred during visualization: {viz_err}\")\n",
    "# else:\n",
    "#     print(\"Error: Workflow has not been compiled yet (variable 'app' not found or is None).\")\n",
    "#     print(\"Please run the cell containing 'app = workflow.compile()' first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
