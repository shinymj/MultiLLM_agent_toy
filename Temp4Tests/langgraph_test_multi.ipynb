{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claude 생성 코드, multi LLM이 동시에 같은 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph\n",
    "import asyncio\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your state structure\n",
    "class State(TypedDict):\n",
    "    prompt_index: int\n",
    "    prompts: List[Dict[str, Any]]\n",
    "    results: Dict[str, List[Any]]\n",
    "    current_prompt: Dict[str, Any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to invoke each model\n",
    "async def invoke_gemma3(state: State):\n",
    "    current_prompt = state[\"current_prompt\"]\n",
    "    system_prompt = current_prompt.get(\"system_prompt\", \"\")\n",
    "    user_input = current_prompt.get(\"input\", \"\")\n",
    "    \n",
    "    # Set up and invoke Gemma 3 model (adjust parameters as needed)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b\", torch_dtype=torch.bfloat16)\n",
    "    \n",
    "    # Format input according to Gemma's requirements\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    formatted_input = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = tokenizer(formatted_input, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Add to results\n",
    "    return {\"results\": {**state[\"results\"], \"gemma3\": state[\"results\"].get(\"gemma3\", []) + [response]}}\n",
    "\n",
    "async def invoke_llama3(state: State):\n",
    "    current_prompt = state[\"current_prompt\"]\n",
    "    system_prompt = current_prompt.get(\"system_prompt\", \"\")\n",
    "    user_input = current_prompt.get(\"input\", \"\")\n",
    "    \n",
    "    # Set up and invoke Llama 3.1 model (adjust parameters as needed)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3-8B\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3-8B\", torch_dtype=torch.bfloat16)\n",
    "    \n",
    "    # Format input according to Llama's requirements\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    formatted_input = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = tokenizer(formatted_input, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Add to results\n",
    "    return {\"results\": {**state[\"results\"], \"llama3\": state[\"results\"].get(\"llama3\", []) + [response]}}\n",
    "\n",
    "async def invoke_deepseek(state: State):\n",
    "    current_prompt = state[\"current_prompt\"]\n",
    "    system_prompt = current_prompt.get(\"system_prompt\", \"\")\n",
    "    user_input = current_prompt.get(\"input\", \"\")\n",
    "    \n",
    "    # Set up and invoke DeepSeek-R1 model (adjust parameters as needed)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-7b-base\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-7b-base\", torch_dtype=torch.bfloat16)\n",
    "    \n",
    "    # Format input according to DeepSeek's requirements\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    formatted_input = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = tokenizer(formatted_input, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Add to results\n",
    "    return {\"results\": {**state[\"results\"], \"deepseek\": state[\"results\"].get(\"deepseek\", []) + [response]}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to select the next prompt or end\n",
    "def should_continue(state: State) -> str:\n",
    "    if state[\"prompt_index\"] >= len(state[\"prompts\"]) - 1:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "# Define a function to prepare the next prompt\n",
    "def prepare_next_prompt(state: State) -> State:\n",
    "    next_index = state[\"prompt_index\"] + 1\n",
    "    next_prompt = state[\"prompts\"][next_index]\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"prompt_index\": next_index,\n",
    "        \"current_prompt\": next_prompt\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다시 생성한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "o3_mini = init_chat_model(\"openai:o3-mini\")\n",
    "claude_sonnet = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0)\n",
    "gemma3 = init_chat_model(\"ollama:gemma3:12b\", temperature=0)\n",
    "llama3_1 = init_chat_model(\"ollama:llama3.1:latest\", temperature=0)\n",
    "deepseek_r1 = init_chat_model(\"ollama:deepseek-r1:8b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "file_path = \"biggen_bench_instruction_idx0.json\"\n",
    "\n",
    "def load_benchmark_data(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Load benchmark data from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def prepare_prompts(benchmark_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Prepare prompts by excluding reference_answer and score_rubric.\"\"\"\n",
    "    prompts = []\n",
    "    for item in benchmark_data:\n",
    "        prompt = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"capability\": item[\"capability\"],\n",
    "            \"task\": item[\"task\"],\n",
    "            \"instance_idx\": item[\"instance_idx\"],\n",
    "            \"system_prompt\": item[\"system_prompt\"],\n",
    "            \"input\": item[\"input\"],\n",
    "            # Exclude reference_answer and score_rubric\n",
    "        }\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "def prepare_rubric(benchmark_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Prepare rubric including reference_answer and score_rubric.\"\"\"\n",
    "    rubric = []\n",
    "    for item in benchmark_data:\n",
    "        prompt = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"reference_answer\": item[\"reference_answer\"],\n",
    "            \"score_rubric\": item[\"score_rubric\"]\n",
    "        }\n",
    "        rubric.append(prompt)\n",
    "    return rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_data = load_benchmark_data(file_path)\n",
    "prompts = prepare_prompts(benchmark_data)\n",
    "rubric = prepare_rubric(benchmark_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'instruction_following_multi_task_inference_0',\n",
       " 'reference_answer': \"Terms referring to Elon Musk:\\n1. Elon Musk\\n2. The world's richest man\\n3. Musk\\n4. He\\n5. Him\\nRephrased terms:\\n1. Elon Musk\\n2. The wealthiest individual\\n3. The entrepreneur\\n4. He\\n5. Him\",\n",
       " 'score_rubric': {'criteria': \"Does the response accurately list all terms referring to Elon Musk and rephrase each of them effectively, excluding pronouns and 'Elon Musk'?\",\n",
       "  'score1_description': 'The response fails to list any terms referring to Elon Musk or lists terms inaccurately, and does not rephrase any terms.',\n",
       "  'score2_description': 'The response lists some but not all terms referring to Elon Musk, rephrases one or more terms inaccurately, or misses rephrasing some listed terms.',\n",
       "  'score3_description': 'The response lists most terms referring to Elon Musk and rephrases them, but with minor inaccuracies or omissions in rephrasing.',\n",
       "  'score4_description': 'The response accurately lists all terms referring to Elon Musk and rephrases them correctly, but the rephrasings may lack creativity or variety.',\n",
       "  'score5_description': \"The response accurately lists all terms referring to Elon Musk and rephrases each term effectively and creatively, fully meeting the instruction's requirements.\"}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rubric[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import asyncio\n",
    "\n",
    "# Define the state structure\n",
    "class State(TypedDict):\n",
    "    prompts: List[Dict[str, Any]]\n",
    "    processed_count: int\n",
    "    gemma3_results: List[Dict[str, Any]]\n",
    "    llama3_1_results: List[Dict[str, Any]]\n",
    "    deepseek_r1_results: List[Dict[str, Any]]\n",
    "\n",
    "# Define model processing functions for each model\n",
    "def process_gemma3(state: State) -> State:\n",
    "    model = gemma3  # Your initialized gemma3 model\n",
    "    model_name = \"gemma3\"\n",
    "    \n",
    "    if state[\"processed_count\"] >= len(state[\"prompts\"]):\n",
    "        return state\n",
    "    \n",
    "    prompt = state[\"prompts\"][state[\"processed_count\"]]\n",
    "    system_prompt = prompt.get('system_prompt', '')\n",
    "    user_input = prompt.get('input', '')\n",
    "    \n",
    "    try:\n",
    "        response = model.invoke(\n",
    "            user_input,\n",
    "            config={\"system_prompt\": system_prompt}\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            \"id\": prompt.get('id', ''),\n",
    "            \"model_name\": model_name,\n",
    "            \"response\": response\n",
    "        }\n",
    "    except Exception as e:\n",
    "        result = {\n",
    "            \"id\": prompt.get('id', ''),\n",
    "            \"model_name\": model_name,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"gemma3_results\": state[\"gemma3_results\"] + [result]\n",
    "    }\n",
    "\n",
    "def process_llama3_1(state: State) -> State:\n",
    "    model = llama3_1  # Your initialized llama3_1 model\n",
    "    model_name = \"llama3_1\"\n",
    "    \n",
    "    if state[\"processed_count\"] >= len(state[\"prompts\"]):\n",
    "        return state\n",
    "    \n",
    "    prompt = state[\"prompts\"][state[\"processed_count\"]]\n",
    "    system_prompt = prompt.get('system_prompt', '')\n",
    "    user_input = prompt.get('input', '')\n",
    "    \n",
    "    try:\n",
    "        response = model.invoke(\n",
    "            user_input,\n",
    "            config={\"system_prompt\": system_prompt}\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            \"id\": prompt.get('id', ''),\n",
    "            \"model_name\": model_name,\n",
    "            \"response\": response\n",
    "        }\n",
    "    except Exception as e:\n",
    "        result = {\n",
    "            \"id\": prompt.get('id', ''),\n",
    "            \"model_name\": model_name,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"llama3_1_results\": state[\"llama3_1_results\"] + [result]\n",
    "    }\n",
    "\n",
    "def process_deepseek_r1(state: State) -> State:\n",
    "    model = deepseek_r1  # Your initialized deepseek_r1 model\n",
    "    model_name = \"deepseek_r1\"\n",
    "    \n",
    "    if state[\"processed_count\"] >= len(state[\"prompts\"]):\n",
    "        return state\n",
    "    \n",
    "    prompt = state[\"prompts\"][state[\"processed_count\"]]\n",
    "    system_prompt = prompt.get('system_prompt', '')\n",
    "    user_input = prompt.get('input', '')\n",
    "    \n",
    "    try:\n",
    "        response = model.invoke(\n",
    "            user_input,\n",
    "            config={\"system_prompt\": system_prompt}\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            \"id\": prompt.get('id', ''),\n",
    "            \"model_name\": model_name,\n",
    "            \"response\": response\n",
    "        }\n",
    "    except Exception as e:\n",
    "        result = {\n",
    "            \"id\": prompt.get('id', ''),\n",
    "            \"model_name\": model_name,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"deepseek_r1_results\": state[\"deepseek_r1_results\"] + [result]\n",
    "    }\n",
    "\n",
    "# Define a function to merge results and increment counter\n",
    "def merge_and_continue(states: List[Dict]) -> Dict:\n",
    "    # Start with the first state\n",
    "    base_state = states[0]\n",
    "    \n",
    "    # Update with results from other states\n",
    "    for state in states[1:]:\n",
    "        if \"gemma3_results\" in state and state[\"gemma3_results\"]:\n",
    "            base_state[\"gemma3_results\"] = state[\"gemma3_results\"]\n",
    "        if \"llama3_1_results\" in state and state[\"llama3_1_results\"]:\n",
    "            base_state[\"llama3_1_results\"] = state[\"llama3_1_results\"]\n",
    "        if \"deepseek_r1_results\" in state and state[\"deepseek_r1_results\"]:\n",
    "            base_state[\"deepseek_r1_results\"] = state[\"deepseek_r1_results\"]\n",
    "    \n",
    "    # Increment the processed count\n",
    "    return {\n",
    "        **base_state,\n",
    "        \"processed_count\": base_state[\"processed_count\"] + 1\n",
    "    }\n",
    "\n",
    "# Define a function to check if we're done processing\n",
    "def check_completion(state: State) -> str:\n",
    "    if state[\"processed_count\"] >= len(state[\"prompts\"]):\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "# Set up the graph\n",
    "def create_parallel_processing_graph():\n",
    "    workflow = StateGraph(State)\n",
    "    \n",
    "    # Add the nodes\n",
    "    workflow.add_node(\"process_gemma3\", process_gemma3)\n",
    "    workflow.add_node(\"process_llama3_1\", process_llama3_1)\n",
    "    workflow.add_node(\"process_deepseek_r1\", process_deepseek_r1)\n",
    "    \n",
    "    # Set up branching\n",
    "    workflow.add_edge(\"start\", (\"process_gemma3\", \"process_llama3_1\", \"process_deepseek_r1\"))\n",
    "    workflow.add_edge((\"process_gemma3\", \"process_llama3_1\", \"process_deepseek_r1\"), merge_and_continue)\n",
    "    \n",
    "    # Set up conditional continuation\n",
    "    workflow.add_conditional_edges(\n",
    "        merge_and_continue,\n",
    "        check_completion,\n",
    "        {\n",
    "            \"continue\": (\"process_gemma3\", \"process_llama3_1\", \"process_deepseek_r1\"),\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Compile the graph\n",
    "    return workflow.compile()\n",
    "\n",
    "# Execute the graph\n",
    "def process_all_prompts_parallel(prompts):\n",
    "    # Create initial state\n",
    "    initial_state = {\n",
    "        \"prompts\": prompts,\n",
    "        \"processed_count\": 0,\n",
    "        \"gemma3_results\": [],\n",
    "        \"llama3_1_results\": [],\n",
    "        \"deepseek_r1_results\": []\n",
    "    }\n",
    "    \n",
    "    # Create and run the graph\n",
    "    graph = create_parallel_processing_graph()\n",
    "    final_state = graph.invoke(initial_state)\n",
    "    \n",
    "    # Return all results\n",
    "    return {\n",
    "        \"gemma3_results\": final_state[\"gemma3_results\"],\n",
    "        \"llama3_1_results\": final_state[\"llama3_1_results\"],\n",
    "        \"deepseek_r1_results\": final_state[\"deepseek_r1_results\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model_processor(model, model_name):\n",
    "    \"\"\"Create a processing function for the given model.\"\"\"\n",
    "    def process_model(state: State) -> State:\n",
    "        if state[\"processed_count\"] >= len(state[\"prompts\"]):\n",
    "            return state\n",
    "        \n",
    "        prompt = state[\"prompts\"][state[\"processed_count\"]]\n",
    "        system_prompt = prompt.get('system_prompt', '')\n",
    "        user_input = prompt.get('input', '')\n",
    "        \n",
    "        try:\n",
    "            response = model.invoke(\n",
    "                user_input,\n",
    "                config={\"system_prompt\": system_prompt}\n",
    "            )\n",
    "            \n",
    "            result = {\n",
    "                \"id\": prompt.get('id', ''),\n",
    "                \"model_name\": model_name,\n",
    "                \"response\": response\n",
    "            }\n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                \"id\": prompt.get('id', ''),\n",
    "                \"model_name\": model_name,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "        \n",
    "        # Update the results for this specific model\n",
    "        results_key = f\"{model_name}_results\"\n",
    "        return {\n",
    "            **state,\n",
    "            results_key: state.get(results_key, []) + [result]\n",
    "        }\n",
    "    \n",
    "    return process_model\n",
    "\n",
    "# Then use it like this:\n",
    "process_gemma3 = create_model_processor(gemma3, \"gemma3\")\n",
    "process_llama3_1 = create_model_processor(llama3_1, \"llama3_1\")\n",
    "process_deepseek_r1 = create_model_processor(deepseek_r1, \"deepseek_r1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.create_model_processor.<locals>.process_model(state: __main__.State) -> __main__.State>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_gemma3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
