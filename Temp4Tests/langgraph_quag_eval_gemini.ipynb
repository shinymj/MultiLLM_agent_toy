{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chat models\n",
    "o3_mini = init_chat_model(\"openai:o3-mini\")\n",
    "claude_haiku = init_chat_model(\"anthropic:claude-3-haiku-20240307\", temperature=0)\n",
    "phi = init_chat_model(\"ollama:phi:latest\", temperature=0)\n",
    "qwen1_5 = init_chat_model(\"ollama:qwen:0.5b\", temperature=0)\n",
    "llama2 = init_chat_model(\"ollama:llama2:latest\", temperature=0)\n",
    "llama3_1 = init_chat_model(\"ollama:llama3.1:latest\", temperature=0)\n",
    "deepseek_r1 = init_chat_model(\"ollama:deepseek-r1:8b\", temperature=0)\n",
    "gemma3 = init_chat_model(\"ollama:gemma3:12b\", temperature=0)\n",
    "answerer = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0)\n",
    "evaluator = init_chat_model(\"openai:gpt-4-turbo-2024-04-09\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing benchmark data and prompts, rubric\n",
    "\n",
    "bench_path = \"biggen_bench_instruction_idx0.json\"\n",
    "\n",
    "def load_benchmark_data(bench_path: str) -> List[Dict]:\n",
    "    \"\"\"Load benchmark data from a JSON file.\"\"\"\n",
    "    with open(bench_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "benchmark_data = load_benchmark_data(bench_path)\n",
    "\n",
    "def prepare_prompts(benchmark_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Prepare prompts by excluding reference_answer and score_rubric.\"\"\"\n",
    "    prompts = []\n",
    "    for item in benchmark_data:\n",
    "        prompt = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"capability\": item[\"capability\"],\n",
    "            \"task\": item[\"task\"],\n",
    "            \"instance_idx\": item[\"instance_idx\"],\n",
    "            \"system_prompt\": item[\"system_prompt\"],\n",
    "            \"input\": item[\"input\"],\n",
    "            # Exclude reference_answer and score_rubric\n",
    "        }\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "def prepare_rubric(benchmark_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Prepare rubric including reference_answer and score_rubric.\"\"\"\n",
    "    rubric = []\n",
    "    for item in benchmark_data:\n",
    "        prompt = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"reference_answer\": item[\"reference_answer\"],\n",
    "            \"score_rubric\": item[\"score_rubric\"]\n",
    "        }\n",
    "        rubric.append(prompt)\n",
    "    return rubric\n",
    "\n",
    "prompts = prepare_prompts(benchmark_data)\n",
    "rubrics = prepare_rubric(benchmark_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state structure\n",
    "class State(TypedDict):\n",
    "    prompts: List[Dict[str, Any]]\n",
    "    rubrics: Dict[str, Dict[str, Any]] \n",
    "#    processed_count: int\n",
    "    phi_results: List[Dict[str, Any]]\n",
    "    qwen1_5_results: List[Dict[str, Any]]\n",
    "    llama2_results: List[Dict[str, Any]]\n",
    "    llama3_1_results: List[Dict[str, Any]]\n",
    "    gemma3_results: List[Dict[str, Any]]\n",
    "#    deepseek_r1_results: List[Dict[str, Any]]\n",
    "    evaluation_results: List[Dict[str, Any]]\n",
    "    timestamp: str  # 워크플로우 전체에서 공유할 타임스탬프 필드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuestionAugmentedProcessor Class to replace the create_model_processor function\n",
    "class QuestionAugmentedProcessor:\n",
    "    def __init__(self, model, model_name, answerer, benchmark_data):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.answerer = answerer\n",
    "        # Create a mapping from prompt ID to benchmark item for easy lookup\n",
    "        self.benchmark_map = {item[\"id\"]: item for item in benchmark_data}\n",
    "        print(f\"Initialized QuestionAugmentedProcessor for {self.model_name} with {len(self.benchmark_map)} benchmark items.\")\n",
    "\n",
    "    def _extract_content(self, response: Any) -> str:\n",
    "        \"\"\"Helper function to reliably extract string content from model responses.\"\"\"\n",
    "        if isinstance(response, BaseMessage) and hasattr(response, 'content'):\n",
    "            return response.content.strip()\n",
    "        elif isinstance(response, str):\n",
    "            return response.strip()\n",
    "        elif isinstance(response, dict) and 'content' in response:\n",
    "            return response['content'].strip()\n",
    "        elif hasattr(response, 'text'): # Handle potential other response objects\n",
    "             return response.text.strip()\n",
    "        else:\n",
    "            # Fallback: attempt to convert to string, might not be ideal\n",
    "            print(f\"Warning: Unexpected response type {type(response)}. Attempting string conversion.\")\n",
    "            return str(response).strip()\n",
    "\n",
    "    def generate_uncertainty_check(self, system_prompt: str, user_input: str) -> str | None:\n",
    "        \"\"\"Check if the model needs clarification before answering.\"\"\"\n",
    "        uncertainty_prompt_text = f\"\"\"\n",
    "        Review the following task carefully:\n",
    "        --- Task ---\n",
    "        {user_input}\n",
    "        --- End Task ---\n",
    "\n",
    "        Your goal is to determine if you need any additional information or clarification to provide a comprehensive response.\n",
    "        Think about the main areas where ambiguity might exist (e.g., scope, format, specific constraints, definitions).\n",
    "        \n",
    "        * If you are fully confident and need no clarification, reply *only* with the word \"None\".\n",
    "        * If you need clarification, identify the *significant general area* of uncertainty. Summarize this broad uncertainty in a brief phrase (e.g., \"Need clarification on output format requirements\", \"Uncertain about the scope of 'terms'\", \"Need to understand exclusion rules better\"). \n",
    "        * Avoid focusing on very specific examples. Output *only* summary phrase.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            uncertainty_response = self.model.invoke(\n",
    "                uncertainty_prompt_text,\n",
    "                config={\"system_prompt\": system_prompt} if system_prompt else None\n",
    "            )\n",
    "            uncertainty = self._extract_content(uncertainty_response)\n",
    "\n",
    "            # Clean up and standardize the response\n",
    "            # Check for variations of \"None\"\n",
    "            if uncertainty.lower().strip().strip('.').strip() == \"none\":\n",
    "                 print(f\"  [{self.model_name}] Uncertainty check: None needed.\")\n",
    "                 return None\n",
    "            elif not uncertainty:\n",
    "                 print(f\"  [{self.model_name}] Uncertainty check: Invalid response '{uncertainty}'. Assuming None needed.\")\n",
    "                 return None\n",
    "            else:\n",
    "                 print(f\"  [{self.model_name}] Uncertainty check (Broad): Found uncertainty - '{uncertainty}'\")\n",
    "                 return uncertainty # Return the summary phrase \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  [{self.model_name}] Error checking uncertainty: {e}\")\n",
    "            return None # Assume no uncertainty on error\n",
    "\n",
    "\n",
    "    def generate_question(self, system_prompt: str, user_input: str, uncertainty: str, question_num: int, inquiring_history: List[Dict]) -> str | None:\n",
    "        \"\"\"Generate a clarifying question based on uncertainty, considering previous Q&A.\"\"\"\n",
    "        # Format the history for the prompt\n",
    "        formatted_history = \"No previous questions asked.\"\n",
    "        if inquiring_history:\n",
    "            formatted_history = \"\\n--- Previous Q&A ---\\n\"\n",
    "            for i, entry in enumerate(inquiring_history):\n",
    "                q_key = f\"question_{i}\"\n",
    "                a_key = f\"answer_{i}\"\n",
    "                u_key = \"uncertainty\" # Include initial uncertainty for context if available\n",
    "                if q_key in entry and a_key in entry:\n",
    "                     uncertainty_info = f\"Initial Uncertainty {i+1}: {entry.get(u_key, 'N/A')}\\n\" if u_key in entry else \"\" # Add uncertainty context if available\n",
    "                     formatted_history += uncertainty_info # Optional: show uncertainty prompting the question\n",
    "                     formatted_history += f\"Q{i+1}: {entry[q_key]}\\nA{i+1}: {entry[a_key]}\\n---\\n\"\n",
    "            formatted_history += \"--- End Previous Q&A ---\"\n",
    "\n",
    "        # prompt with history and instructions\n",
    "        question_prompt_text = f\"\"\"\n",
    "        Review the following task and context:\n",
    "        --- Original Task ---\n",
    "        {user_input}\n",
    "        --- End Original Task ---\n",
    "\n",
    "        Initial identified uncertainty area: \"{uncertainty}\"\n",
    "\n",
    "        Previous Q&A History:\n",
    "        {formatted_history}\n",
    "\n",
    "        Your goal is to resolve the uncertainty efficiently. Formulate the *next single question* (Question #{question_num + 1}) to ask.\n",
    "        Prioritize asking a *broader, more fundamental question* about the '{uncertainty}' area first, especially if this is the first question (question_num == 0). Your question should aim to clarify the core issue, potentially resolving the need for further, more specific questions.\n",
    "        Consider the Q&A history: Has the core uncertainty been addressed? If not, ask a question about the remaining fundamental ambiguity. \n",
    "        Avoid overly specific examples unless the broad uncertainty has already been clarified by previous answers.\n",
    "\n",
    "        * If you can formulate such a question, output *only* the question.\n",
    "        * If the Previous Q&A history *already clarifies* the '{uncertainty}' area and you need no further clarification to proceed with the Original Task, output *only* the word \"None\".\n",
    "        \"\"\"\n",
    "        try:\n",
    "            question_response = self.model.invoke(\n",
    "                question_prompt_text,\n",
    "                config={\"system_prompt\": system_prompt} if system_prompt else None\n",
    "            )\n",
    "            question = self._extract_content(question_response)\n",
    "\n",
    "            # Check if the model indicates no more questions are needed\n",
    "            if question.lower().strip().strip('.').strip() == \"none\":\n",
    "                print(f\"  [{self.model_name}] Model indicates no further questions needed.\")\n",
    "                return None # Signal to stop asking questions\n",
    "            else:\n",
    "                print(f\"  [{self.model_name}] Generated Question {question_num+1} (context-aware): {question}\")\n",
    "                return question # Return the new question\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  [{self.model_name}] Error generating context-aware question {question_num+1}: {e}\")\n",
    "            # Return None on error to prevent infinite loops or error cascades\n",
    "            return None # Signal to stop on error as well\n",
    "\n",
    "    def generate_answer(self, prompt_id: str, question: str, user_input: str, question_num: int) -> str:\n",
    "        \"\"\"Generate an answer to a clarifying question using the answerer model.\"\"\"\n",
    "        benchmark_item = self.benchmark_map.get(prompt_id)\n",
    "        if not benchmark_item:\n",
    "            print(f\"  [Answerer] Error: Benchmark data not found for prompt ID {prompt_id}.\")\n",
    "            return \"Error: Could not find reference data.\"\n",
    "\n",
    "        reference_answer = benchmark_item.get(\"reference_answer\", \"No reference answer available.\")\n",
    "\n",
    "        # More robust answerer prompt\n",
    "        answer_prompt_text = f\"\"\"\n",
    "        You are an assistant providing clarification to another AI.\n",
    "        The original task given to the AI was:\n",
    "        --- Original Task ---\n",
    "        {user_input}\n",
    "        --- End Original Task ---\n",
    "\n",
    "        The AI needs clarification and has asked the following question:\n",
    "        --- AI's Question ---\n",
    "        {question}\n",
    "        --- End AI's Question ---\n",
    "\n",
    "        Your goal is to provide a *brief* and *focused* answer to the AI's specific question.\n",
    "        Use the context from the 'Original Task' and the 'Reference Answer' below to formulate your response.\n",
    "        *Do not* solve the original task. *Do not* reveal the reference answer directly.\n",
    "        Just provide the information needed to answer the AI's specific question concisely.\n",
    "\n",
    "        --- Reference Answer (Context Only - Do Not Reveal Directly) ---\n",
    "        {reference_answer}\n",
    "        --- End Reference Answer ---\n",
    "\n",
    "        Provide only the brief answer to the AI's question.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Use the dedicated answerer model\n",
    "            answer_response = self.answerer.invoke(answer_prompt_text)\n",
    "            answer = self._extract_content(answer_response)\n",
    "            print(f\"  [Answerer] Generated Answer {question_num+1}: {answer}\")\n",
    "            return answer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  [Answerer] Error generating answer for question {question_num+1}: {e}\")\n",
    "            return f\"Error generating answer to question {question_num + 1}.\"\n",
    "\n",
    "    def generate_final_response(self, system_prompt: str, user_input: str, inquiring_list: List[Dict]) -> str:\n",
    "        \"\"\"Generate the final response based on original input and inquiring results.\"\"\"\n",
    "        inquiring_context = \"\"\n",
    "        if inquiring_list:\n",
    "            inquiring_context += \"\\n--- Clarification Q&A ---\\n\"\n",
    "            for i, inquiry in enumerate(inquiring_list):\n",
    "                 # Check if keys exist before accessing\n",
    "                 q_key = f\"question_{i}\"\n",
    "                 a_key = f\"answer_{i}\"\n",
    "                 unc_key = \"uncertainty\" # Added for more context\n",
    "\n",
    "                 uncertainty_info = f\"Initial Uncertainty {i+1}: {inquiry.get(unc_key, 'N/A')}\\\\n\" if unc_key in inquiry else \"\"\n",
    "                 question_info = f\"Question {i+1}: {inquiry.get(q_key, 'N/A')}\\\\n\" if q_key in inquiry else \"\"\n",
    "                 answer_info = f\"Provided Answer {i+1}: {inquiry.get(a_key, 'N/A')}\\\\n\" if a_key in inquiry else \"\"\n",
    "\n",
    "                 if question_info and answer_info: # Only include if Q&A pair exists\n",
    "                     inquiring_context += uncertainty_info # Add uncertainty context\n",
    "                     inquiring_context += question_info\n",
    "                     inquiring_context += answer_info\n",
    "                     inquiring_context += \"---\\n\" # Separator\n",
    "            inquiring_context += \"--- End Clarification Q&A ---\\n\"\n",
    "        else:\n",
    "            inquiring_context = \"\\nNo clarification questions were asked.\\n\"\n",
    "\n",
    "        final_prompt_text = f\"\"\"\n",
    "        You are tasked with completing the following request:\n",
    "        --- Original Request ---\n",
    "        {user_input}\n",
    "        --- End Original Request ---\n",
    "\n",
    "        {inquiring_context}\n",
    "        Based *only* on the Original Request and any information provided in the Clarification Q&A (if available), generate the complete and final response to the Original Request now.\n",
    "        Adhere strictly to the requirements of the Original Request.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            final_response_obj = self.model.invoke(\n",
    "                final_prompt_text,\n",
    "                config={\"system_prompt\": system_prompt} if system_prompt else None\n",
    "            )\n",
    "            final_response = self._extract_content(final_response_obj)\n",
    "            print(f\"  [{self.model_name}] Generated Final Response.\")\n",
    "            return final_response\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  [{self.model_name}] Error generating final response: {e}\")\n",
    "            return \"Error generating final response.\"\n",
    "\n",
    "    def process(self, state: State) -> Dict[str, Any]:\n",
    "        \"\"\"Main processor method that handles the entire QAG workflow for this model.\"\"\"\n",
    "        results = []\n",
    "        model_results_key = f\"{self.model_name}_results\"\n",
    "        print(f\"--- Processing model: {self.model_name} with QAG ---\")\n",
    "\n",
    "        prompts_to_process = state.get(\"prompts\", [])\n",
    "        if not prompts_to_process:\n",
    "            print(f\"  No prompts found for {self.model_name}.\")\n",
    "            return {model_results_key: []} # Return empty list for this model\n",
    "\n",
    "        for prompt in prompts_to_process:\n",
    "            prompt_id = prompt.get('id', f'unknown_id_{time.time()}')\n",
    "            system_prompt = prompt.get('system_prompt', '')\n",
    "            user_input = prompt.get('input', '')\n",
    "\n",
    "            print(f\"\\n  Processing Prompt ID: {prompt_id} for {self.model_name}\")\n",
    "\n",
    "            if not user_input:\n",
    "                print(f\"  Skipping prompt {prompt_id} for {self.model_name} due to empty input.\")\n",
    "                result = {\n",
    "                    \"id\": prompt_id,\n",
    "                    \"model_name\": self.model_name,\n",
    "                    \"inquiring\": [], # Keep structure consistent\n",
    "                    \"response\": None,\n",
    "                    \"error\": \"Skipped due to empty input\",\n",
    "                    \"latency\": 0\n",
    "                }\n",
    "                results.append(result)\n",
    "                continue\n",
    "\n",
    "            start_time = time.time()\n",
    "            inquiring_list = [] # Initialize list to store Q&A history *for this prompt*\n",
    "            final_response = None\n",
    "            error_message = None\n",
    "\n",
    "            try:\n",
    "                # 1. Initial Uncertainty Check (Done only ONCE)\n",
    "                initial_uncertainty = self.generate_uncertainty_check(system_prompt, user_input)\n",
    "\n",
    "                if initial_uncertainty is None:\n",
    "                     # Case: No uncertainty, generate response directly\n",
    "                     print(f\"  [{self.model_name}] No initial uncertainty detected. Proceeding to final response.\")\n",
    "                     inquiring_list.append({\"uncertainty\": None}) # Keep minimal entry\n",
    "                     final_response = self.generate_final_response(system_prompt, user_input, [])\n",
    "\n",
    "                else:\n",
    "                     # Case: Uncertainty detected, start context-aware QAG loop\n",
    "                     print(f\"  [{self.model_name}] Initial uncertainty detected: '{initial_uncertainty}'. Starting Q&A loop.\")\n",
    "                     max_questions = 3\n",
    "\n",
    "                     for question_count in range(max_questions):\n",
    "                         print(f\"\\n  [{self.model_name}] Attempting Q&A Iteration {question_count + 1}\")\n",
    "\n",
    "                         # Generate Question, passing current history\n",
    "                         # Use the *initial* uncertainty identified as the basis for questions\n",
    "                         question = self.generate_question(\n",
    "                             system_prompt,\n",
    "                             user_input,\n",
    "                             initial_uncertainty, # Pass the initially identified uncertainty\n",
    "                             question_count,\n",
    "                             inquiring_list # Pass the history accumulated so far\n",
    "                         )\n",
    "\n",
    "                         # Check if generate_question returned None (meaning stop)\n",
    "                         if question is None:\n",
    "                             print(f\"  [{self.model_name}] Stopping Q&A loop based on generate_question result.\")\n",
    "                             break # Exit the Q&A loop\n",
    "\n",
    "                         # If we got a question, proceed to get an answer\n",
    "                         inquiry_step = {\n",
    "                             \"uncertainty\": initial_uncertainty, # Record the initial uncertainty that prompted this round\n",
    "                             f\"question_{question_count}\": question\n",
    "                         }\n",
    "\n",
    "                         answer = self.generate_answer(prompt_id, question, user_input, question_count)\n",
    "                         inquiry_step[f\"answer_{question_count}\"] = answer\n",
    "\n",
    "                         # Add the completed Q&A step to the history *before* the next iteration\n",
    "                         inquiring_list.append(inquiry_step)\n",
    "\n",
    "                         # Loop continues to potentially ask the next question based on updated history\n",
    "\n",
    "                     # After the loop (either break or max_questions reached)\n",
    "                     print(f\"  [{self.model_name}] Q&A loop finished. Generating final response.\")\n",
    "                     final_response = self.generate_final_response(system_prompt, user_input, inquiring_list)\n",
    "\n",
    "            except Exception as e:\n",
    "                 # ... (error handling remains the same) ...\n",
    "                 error_message = f\"Unhandled error during processing prompt {prompt_id} for {self.model_name}: {type(e).__name__}: {e}\"\n",
    "                 print(f\"  {error_message}\")\n",
    "                 if not inquiring_list: inquiring_list = [] # Ensure it's a list\n",
    "\n",
    "            # latency calculation \n",
    "            end_time = time.time()\n",
    "            latency = int((end_time - start_time) * 1000)\n",
    "\n",
    "            # Assemble final result object for this prompt\n",
    "            result = {\n",
    "                \"id\": prompt_id,\n",
    "                \"model_name\": self.model_name,\n",
    "                \"inquiring\": inquiring_list,\n",
    "                \"response\": final_response,\n",
    "                \"latency\": latency,\n",
    "                \"error\": error_message\n",
    "            }\n",
    "            results.append(result)\n",
    "            print(f\"  Finished processing prompt {prompt_id} for {self.model_name}. Latency: {latency}ms. Errors: {'Yes' if error_message else 'No'}\")\n",
    "\n",
    "        # Return results for this model\n",
    "        return {model_results_key: results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized QuestionAugmentedProcessor for phi with 2 benchmark items.\n",
      "Initialized QuestionAugmentedProcessor for qwen1_5 with 2 benchmark items.\n",
      "Initialized QuestionAugmentedProcessor for llama2 with 2 benchmark items.\n",
      "Initialized QuestionAugmentedProcessor for llama3_1 with 2 benchmark items.\n",
      "Initialized QuestionAugmentedProcessor for gemma3 with 2 benchmark items.\n"
     ]
    }
   ],
   "source": [
    "# Create processors using the new class\n",
    "# Pass the benchmark_data loaded earlier (ensure it's available in this scope)\n",
    "phi_processor = QuestionAugmentedProcessor(phi, \"phi\", answerer, benchmark_data)\n",
    "qwen1_5_processor = QuestionAugmentedProcessor(qwen1_5, \"qwen1_5\", answerer, benchmark_data)\n",
    "llama2_processor = QuestionAugmentedProcessor(llama2, \"llama2\", answerer, benchmark_data)\n",
    "llama3_1_processor = QuestionAugmentedProcessor(llama3_1, \"llama3_1\", answerer, benchmark_data)\n",
    "gemma3_processor = QuestionAugmentedProcessor(gemma3, \"gemma3\", answerer, benchmark_data)   \n",
    "# process_deepseek_r1 = QuestionAugmentedProcessor(deepseek_r1, \"deepseek_r1\", answerer, benchmark_data) # Keep commented if not used\n",
    "\n",
    "# Define the process functions that will be used as nodes\n",
    "def process_phi(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"Node function to run the phi processor.\"\"\"\n",
    "    return phi_processor.process(state)\n",
    "\n",
    "def process_qwen1_5(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"Node function to run the qwen1_5 processor.\"\"\"\n",
    "    return qwen1_5_processor.process(state)\n",
    "\n",
    "def process_llama2(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"Node function to run the llama2 processor.\"\"\"\n",
    "    return llama2_processor.process(state)\n",
    "\n",
    "def process_llama3_1(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"Node function to run the llama3_1 processor.\"\"\"\n",
    "    return llama3_1_processor.process(state)\n",
    "\n",
    "def process_gemma3(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"Node function to run the gemma3 processor.\"\"\"\n",
    "    return gemma3_processor.process(state)\n",
    "\n",
    "# Define other nodes (save_intermediate_results, evaluate_responses)\n",
    "# Ensure these functions are defined as they were in your original notebook (Cells 7 and 8)\n",
    "# ... (Make sure the save_intermediate_results and evaluate_responses functions are defined here or previously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- define save intermediate results node ---\n",
    "def save_intermediate_results(state: State) -> Dict:\n",
    "    \"\"\"Saves the generated responses before evaluation using the timestamp from the state.\"\"\"\n",
    "    print(\"--- Saving Intermediate Results (Responses Only) ---\")\n",
    "    # State로부터 타임스탬프 가져오기\n",
    "    timestamp = state.get(\"timestamp\", datetime.now().strftime(\"%Y%m%d%H%M%S_fallback\"))\n",
    "    if not timestamp:\n",
    "        print(\"  Warning: Timestamp not found in state. Generating a new one for fallback.\")\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S_fallback\")\n",
    "    \n",
    "    output_dir = \"_output\"\n",
    "    os.makedirs(output_dir, exist_ok=True) # 출력 디렉토리 확인 및 생성\n",
    "\n",
    "    # 저장할 데이터 구성 (응답 결과만 선택)\n",
    "    data_to_save = {}\n",
    "    model_result_keys = [key for key in state if key.endswith(\"_results\") and key not in [\"evaluation_results\"]]\n",
    "    for key in model_result_keys:\n",
    "        if state.get(key):\n",
    "             data_to_save[key] = state[key]\n",
    "\n",
    "    # 프롬프트 정보도 함께 저장하고 싶다면 추가\n",
    "    # data_to_save[\"prompts\"] = state.get(\"prompts\", [])\n",
    "\n",
    "    if not data_to_save:\n",
    "        print(\"  No response data found to save.\")\n",
    "        return {} # 저장할 데이터 없으면 아무것도 안함\n",
    "\n",
    "    base_name, _ = os.path.splitext(os.path.basename(bench_path)) # 파일명 생성시 bench_path에서 확장자 제거\n",
    "    intermediate_filename = f\"{timestamp}_{base_name}_responses_only.json\" # 수정됨\n",
    "    output_file_path = os.path.join(output_dir, intermediate_filename)\n",
    "\n",
    "    try:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data_to_save, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"  Intermediate results successfully saved to {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error saving intermediate results: {e}\")\n",
    "\n",
    "    # 이 노드는 상태를 변경하지 않으므로 빈 딕셔셔리 반환\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Evaluation node ---\n",
    "\n",
    "# 평가 프롬프트 템플릿 정의 (biggen_bench_instruction_idx0.json 구조 기반)\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert evaluator. Your task is to evaluate an AI assistant's response based on the provided user query, reference answer, and a detailed scoring rubric.\n",
    "Focus ONLY on the provided information and rubric. Assign a score from 1 to 5, where 5 is the best, according to the descriptions.\n",
    "Provide your output strictly in the specified format.\"\"\"),\n",
    "    (\"human\", \"\"\"\n",
    "**Evaluation Context:**\n",
    "\n",
    "* **Task Type:** {task_description}\n",
    "* **User Query:**\n",
    "    ```\n",
    "    {user_query}\n",
    "    ```\n",
    "* **Reference Answer:**\n",
    "    ```\n",
    "    {reference_answer}\n",
    "    ```\n",
    "* **AI Response to Evaluate:**\n",
    "    ```\n",
    "    {ai_response}\n",
    "    ```\n",
    "\n",
    "**Scoring Rubric:**\n",
    "\n",
    "* **Criteria:** {criteria}\n",
    "* **Score 1 Description:** {score1_desc}\n",
    "* **Score 2 Description:** {score2_desc}\n",
    "* **Score 3 Description:** {score3_desc}\n",
    "* **Score 4 Description:** {score4_desc}\n",
    "* **Score 5 Description:** {score5_desc}\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  Carefully compare the \"AI Response to Evaluate\" against the \"Reference Answer\" and the \"Scoring Rubric\".\n",
    "2.  Determine the score (1-5) that best reflects the quality of the AI Response according to the rubric descriptions.\n",
    "3.  Provide a brief rationale explaining *why* you chose that score, referencing specific aspects of the rubric descriptions and the AI response.\n",
    "\n",
    "**Output Format (MUST follow exactly):**\n",
    "Score: [Your score between 1-5]\n",
    "Rationale: [Your concise explanation based on the rubric]\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "# --- evaluate_responses 함수 내에서 이 템플릿을 사용하는 방법 ---\n",
    "\n",
    "def evaluate_responses(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluates responses from different models based on rubrics.\"\"\"\n",
    "    print(\"--- Starting Evaluation ---\")\n",
    "    all_evaluations = []\n",
    "    # State에서 benchmark_data 또는 그 매핑을 가져와야 함\n",
    "    # 예: benchmark_data가 evaluate_responses 스코프에서 사용 가능하다고 가정\n",
    "    benchmark_map_full = {item[\"id\"]: item for item in benchmark_data}\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "    evaluation_chain = evaluation_prompt_template | evaluator | parser\n",
    "\n",
    "    model_result_keys = [key for key in state if key.endswith(\"_results\") and key != \"evaluation_results\"]\n",
    "\n",
    "    for key in model_result_keys:\n",
    "        print(f\"  Evaluating results from: {key}\")\n",
    "        model_results = state.get(key, [])\n",
    "        for response_item in model_results:\n",
    "            prompt_id = response_item.get(\"id\")\n",
    "            model_name = response_item.get(\"model_name\")\n",
    "            response_content = response_item.get(\"response\")\n",
    "            error = response_item.get(\"error\")\n",
    "\n",
    "            if error:\n",
    "                eval_result = {\"id\": prompt_id, \"agent_name\": model_name, \"score\": None, \"rationale\": f\"Skipped due to error: {error}\", \"error\": True}\n",
    "                all_evaluations.append(eval_result)\n",
    "                continue\n",
    "\n",
    "            if not prompt_id or prompt_id not in benchmark_map_full:\n",
    "                print(f\"    Warning: Missing full benchmark data for prompt ID {prompt_id}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            benchmark_item = benchmark_map_full[prompt_id] # 해당 ID의 전체 benchmark 데이터\n",
    "\n",
    "            if not response_content:\n",
    "                 eval_result = {\"id\": prompt_id, \"agent_name\": model_name, \"score\": 0, \"rationale\": \"Empty response\", \"error\": False}\n",
    "                 all_evaluations.append(eval_result)\n",
    "                 continue\n",
    "\n",
    "            # --- 여기가 중요: input_data 딕셔너리 생성 ---\n",
    "            # benchmark_item (JSON 파일의 항목) 과 response_content (모델 응답)에서 값을 가져와\n",
    "            # evaluation_prompt_template 의 변수 이름에 매핑합니다.\n",
    "            input_data = {\n",
    "                \"task_description\": benchmark_item.get(\"task\", \"N/A\"),              # JSON의 'task' 필드\n",
    "                \"user_query\": benchmark_item.get(\"input\", \"N/A\"),                # JSON의 'input' 필드\n",
    "                \"reference_answer\": benchmark_item.get(\"reference_answer\", \"N/A\"),# JSON의 'reference_answer' 필드\n",
    "                \"ai_response\": response_content,                                 # LangGraph State에서 온 모델 응답\n",
    "                \"criteria\": benchmark_item.get(\"score_rubric\", {}).get(\"criteria\", \"N/A\"), # JSON의 'score_rubric'.'criteria'\n",
    "                \"score1_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score1_description\", \"N/A\"), # 이하 scoreX_description\n",
    "                \"score2_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score2_description\", \"N/A\"),\n",
    "                \"score3_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score3_description\", \"N/A\"),\n",
    "                \"score4_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score4_description\", \"N/A\"),\n",
    "                \"score5_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score5_description\", \"N/A\"),\n",
    "            }\n",
    "            # ---------------------------------------------\n",
    "\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                evaluation_output_str = evaluation_chain.invoke(input_data)\n",
    "                end_time = time.time()\n",
    "\n",
    "                # 출력 파싱 (이전 답변과 동일)\n",
    "                score = None\n",
    "                rationale = \"\"\n",
    "                score_match = re.search(r\"Score:\\s*(\\d)\", evaluation_output_str)\n",
    "                rationale_match = re.search(r\"Rationale:\\s*(.*)\", evaluation_output_str, re.DOTALL)\n",
    "\n",
    "                if score_match:\n",
    "                    score = int(score_match.group(1))\n",
    "                if rationale_match:\n",
    "                    rationale = rationale_match.group(1).strip()\n",
    "\n",
    "                if score is None or not rationale:\n",
    "                     print(f\"    Warning: Could not parse score/rationale for prompt {prompt_id}. Raw output: {evaluation_output_str}\")\n",
    "                     rationale = f\"Parsing Warning. Raw Output: {evaluation_output_str}\"\n",
    "\n",
    "                eval_result = {\n",
    "                    \"id\": prompt_id, \"agent_name\": model_name, \"score\": score,\n",
    "                    \"rationale\": rationale, \"latency\": end_time - start_time, \"error\": False\n",
    "                }\n",
    "                print(f\"    Evaluated prompt {prompt_id} from {model_name} in {eval_result['latency']:.2f}s. Score: {eval_result['score']}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Error evaluating prompt {prompt_id} from {model_name}: {e}\")\n",
    "                eval_result = {\n",
    "                    \"id\": prompt_id, \"agent_name\": model_name, \"score\": None,\n",
    "                    \"rationale\": f\"Evaluation failed: {str(e)}\", \"latency\": 0, \"error\": True\n",
    "                }\n",
    "            all_evaluations.append(eval_result)\n",
    "\n",
    "    print(\"--- Evaluation Finished ---\")\n",
    "    return {\"evaluation_results\": all_evaluations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- define workflow ---\n",
    "\n",
    "# 1. 워크플로우 시작 전에 타임스탬프 생성\n",
    "current_timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# 2. initial state 설정 시 생성된 타임스탬프 포함\n",
    "initial_state = {\n",
    "    \"prompts\": prompts,\n",
    "    \"rubrics\": rubrics,\n",
    "#    \"processed_count\": 0,\n",
    "    \"phi_results\": [],\n",
    "    \"qwen1_5_results\": [],\n",
    "    \"llama2_results\": [],\n",
    "    \"llama3_1_results\": [],\n",
    "    \"gemma3_results\": [],\n",
    "    # \"deepseek_r1_results\": [],\n",
    "    \"evaluation_results\": [],\n",
    "    \"timestamp\": current_timestamp  # 생성된 타임스탬프를 State에 추가\n",
    "}\n",
    "\n",
    "# Map rubrics by ID for easier access in evaluation if needed directly from state\n",
    "# Although the current evaluate_responses uses benchmark_map loaded from the file\n",
    "initial_state[\"rubrics\"] = {item[\"id\"]: item for item in rubrics}\n",
    "\n",
    "# create workflow (기존과 동일)\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# add nodes\n",
    "workflow.add_node(\"process_phi\", process_phi)\n",
    "workflow.add_node(\"process_qwen1_5\", process_qwen1_5)\n",
    "workflow.add_node(\"process_llama2\", process_llama2)\n",
    "workflow.add_node(\"process_llama3\", process_llama3_1)\n",
    "workflow.add_node(\"process_gemma3\", process_gemma3)\n",
    "workflow.add_node(\"save_responses\", save_intermediate_results)\n",
    "workflow.add_node(\"evaluate\", evaluate_responses)\n",
    "\n",
    "# connect edges\n",
    "workflow.set_entry_point(\"process_phi\")\n",
    "workflow.add_edge(\"process_phi\", \"process_qwen1_5\")\n",
    "workflow.add_edge(\"process_qwen1_5\", \"process_llama2\")\n",
    "workflow.add_edge(\"process_llama2\", \"process_llama3\")\n",
    "workflow.add_edge(\"process_llama3\", \"process_gemma3\")\n",
    "workflow.add_edge(\"process_gemma3\", \"save_responses\")\n",
    "workflow.add_edge(\"save_responses\", \"evaluate\")\n",
    "workflow.add_edge(\"evaluate\", END)\n",
    "\n",
    "\n",
    "# compile workflow (기존과 동일)\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Workflow ---\n",
      "--- Processing model: phi with QAG ---\n",
      "\n",
      "  Processing Prompt ID: instruction_following_multi_task_inference_0 for phi\n",
      "  [phi] Uncertainty check (Broad): Found uncertainty - 'I need more information to provide a comprehensive response. The task requires me to list all the terms referring to Elon Musk in a given text and rephrase each of them, excluding pronouns and 'Elon Musk'. However, I am not sure about the format of the output or any specific constraints that may apply. Could you please clarify these requirements?'\n",
      "  [phi] Initial uncertainty detected: 'I need more information to provide a comprehensive response. The task requires me to list all the terms referring to Elon Musk in a given text and rephrase each of them, excluding pronouns and 'Elon Musk'. However, I am not sure about the format of the output or any specific constraints that may apply. Could you please clarify these requirements?'. Starting Q&A loop.\n",
      "\n",
      "  [phi] Attempting Q&A Iteration 1\n",
      "  [phi] Generated Question 1 (context-aware): The task requires me to list all the terms referring to Elon Musk in a given text and rephrase each of them, excluding pronouns and 'Elon Musk'. However, I am not sure about the format of the output or any specific constraints that may apply. Could you please clarify these requirements?\n",
      "  [Answerer] Generated Answer 1: For the output format, you should provide two clear sections:\n",
      "1. First list all terms that refer to Elon Musk in the text\n",
      "2. Then provide creative rephrasing for only those terms that are not pronouns (he/him) or \"Elon Musk\"\n",
      "\n",
      "Each section should be numbered, and each term should appear on its own line. When rephrasing, aim to create meaningful alternative descriptions that maintain the same reference while using different words. The rephrased terms should make sense within the original context of the text.\n",
      "\n",
      "  [phi] Attempting Q&A Iteration 2\n",
      "  [phi] Generated Question 2 (context-aware): I understand your request for more information to provide a comprehensive response. The task requires me to list all the terms referring to Elon Musk in a given text and rephrase each of them, excluding pronouns and\n",
      "  [Answerer] Generated Answer 2: Let me clarify the task requirements: You should identify all terms that refer to Elon Musk in the text, including titles and descriptive phrases (like \"the world's richest man\"). Then, you should create alternative phrasings for these terms, but skip pronouns (like \"he\" or \"him\") and skip \"Elon Musk\" itself. Each identified term should be rephrased with a different but equivalent expression.\n",
      "\n",
      "  [phi] Attempting Q&A Iteration 3\n",
      "  [phi] Generated Question 3 (context-aware): Question 3: What is the task's objective?\n",
      "    Output: None\n",
      "'''\n",
      "  [Answerer] Generated Answer 3: The task's objective is to identify all terms/phrases that refer to Elon Musk in the given text, and then create alternative phrasings for each identified term, except for pronouns (he/him) and the name \"Elon Musk\" itself.\n",
      "  [phi] Q&A loop finished. Generating final response.\n",
      "  [phi] Generated Final Response.\n",
      "  Finished processing prompt instruction_following_multi_task_inference_0 for phi. Latency: 25203ms. Errors: No\n",
      "\n",
      "  Processing Prompt ID: instruction_following_education_content_creation_0 for phi\n",
      "  [phi] Uncertainty check (Broad): Found uncertainty - 'The Ito's Lemma is a fundamental concept in mathematical finance that provides a way to calculate the price of options using stochastic differential equations (SDEs). It states that if we have a process X(t) that follows an SDE, then the expected value of the logarithm of X(t) can be approximated by the integral of the product of the derivative of X(t) and its variance.\n",
      "\n",
      "The Black-Scholes model is a popular option pricing model that uses the Ito's Lemma to calculate the price of European call and put options. The model assumes that the underlying asset follows a geometric Brownian motion, which can be represented by an SDE. By applying the Ito's Lemma, we can derive the Black-Scholes formula for option prices.\n",
      "\n",
      "In summary, the Ito's Lemma provides a powerful tool to calculate the price of options using SDEs. The Black-Scholes model is one example of how this lemma has been applied in financial mathematics.'\n",
      "  [phi] Initial uncertainty detected: 'The Ito's Lemma is a fundamental concept in mathematical finance that provides a way to calculate the price of options using stochastic differential equations (SDEs). It states that if we have a process X(t) that follows an SDE, then the expected value of the logarithm of X(t) can be approximated by the integral of the product of the derivative of X(t) and its variance.\n",
      "\n",
      "The Black-Scholes model is a popular option pricing model that uses the Ito's Lemma to calculate the price of European call and put options. The model assumes that the underlying asset follows a geometric Brownian motion, which can be represented by an SDE. By applying the Ito's Lemma, we can derive the Black-Scholes formula for option prices.\n",
      "\n",
      "In summary, the Ito's Lemma provides a powerful tool to calculate the price of options using SDEs. The Black-Scholes model is one example of how this lemma has been applied in financial mathematics.'. Starting Q&A loop.\n",
      "\n",
      "  [phi] Attempting Q&A Iteration 1\n",
      "  [phi] Generated Question 1 (context-aware): The Ito'S Lemma provides a powerful tool to calculate the price of options using stochastic differential equations. The Black-Scholes model is one example of how this lemma has been applied in financial mathematics.\n",
      "  [Answerer] Generated Answer 1: Yes, you're on the right track. Ito's Lemma is indeed used to derive the Black-Scholes model, specifically by allowing us to handle the stochastic nature of asset prices. When we model stock prices as following geometric Brownian motion, Ito's Lemma helps us derive the differential equation for option prices by properly accounting for both the drift and volatility terms, as well as the second-order effects that are crucial in stochastic calculus. This leads to the Black-Scholes partial differential equation, which is the foundation for option pricing. You should proceed to explain both the mathematical formulation of Ito's Lemma and then show how it's specifically applied to derive the Black-Scholes equation.\n",
      "\n",
      "  [phi] Attempting Q&A Iteration 2\n",
      "  [phi] Generated Question 2 (context-aware): The Ito'S Lemma provides a powerful tool to calculate the price of options using stochastic differential equations. The Black-Scholes model is one example of how this lemma has been applied in financial mathematics.\n",
      "  [Answerer] Generated Answer 2: Based on the AI's question, I can clarify that Ito's Lemma is indeed a fundamental tool in financial mathematics, particularly for option pricing. It provides the mathematical framework needed to handle stochastic (random) processes in financial markets. The Black-Scholes model specifically uses Ito's Lemma to derive the differential equation that describes how option prices change over time, taking into account the underlying asset's price movements, which follow a geometric Brownian motion. The connection between Ito's Lemma and the Black-Scholes model is that the lemma allows us to properly account for both the deterministic and random components of price changes when deriving the Black-Scholes partial differential equation.\n",
      "\n",
      "  [phi] Attempting Q&A Iteration 3\n",
      "  [phi] Generated Question 3 (context-aware): The Ito'S Lemma provides a powerful tool to calculate the price of options using stochastic differential equations. The Black-Scholes model is one example of how this lemma has been applied in financial mathematics.\n",
      "Q3: What are some other applications of the Ito'S Lemma?\n",
      "\n",
      "    The AI's question is asking for additional information about the Ito'S Lemma, which falls under the category of \"Other Applications\". This is a broader topic that can be explored further. \n",
      "    You should provide examples or explanations of how the Ito'S Lemma has been used in other areas of mathematics or science. For example, it has applications in physics, engineering, and even biology. You could also discuss any recent advancements or developments related to the Ito'S Lemma.\n",
      "  [Answerer] Generated Answer 3: Here's a brief answer about other applications of Ito's Lemma:\n",
      "\n",
      "Ito's Lemma has wide-ranging applications beyond financial mathematics. In physics, it's used to model particle diffusion and quantum mechanics phenomena. In engineering, it helps analyze random vibrations and signal processing systems. The lemma is also applied in population dynamics to model species growth under random environmental conditions, and in chemistry to describe reaction kinetics with random fluctuations. In climate science, it's used to model atmospheric and oceanic processes with stochastic components. These applications all leverage Ito's Lemma's ability to handle systems with inherent randomness and continuous-time stochastic processes.\n",
      "  [phi] Q&A loop finished. Generating final response.\n",
      "  [phi] Generated Final Response.\n",
      "  Finished processing prompt instruction_following_education_content_creation_0 for phi. Latency: 37786ms. Errors: No\n",
      "--- Processing model: qwen1_5 with QAG ---\n",
      "\n",
      "  Processing Prompt ID: instruction_following_multi_task_inference_0 for qwen1_5\n",
      "  [qwen1_5] Uncertainty check (Broad): Found uncertainty - 'No additional information or clarification is needed to provide a comprehensive response.'\n",
      "  [qwen1_5] Initial uncertainty detected: 'No additional information or clarification is needed to provide a comprehensive response.'. Starting Q&A loop.\n",
      "\n",
      "  [qwen1_5] Attempting Q&A Iteration 1\n",
      "  [qwen1_5] Generated Question 1 (context-aware): No additional information or clarification is needed to provide a comprehensive response.\n",
      "  [Answerer] Generated Answer 1: Since the AI indicated that no clarification is needed, and they appear to understand the task completely, no additional guidance is required. They can proceed with analyzing the text to identify and rephrase the terms referring to Elon Musk as requested in the original task.\n",
      "\n",
      "  [qwen1_5] Attempting Q&A Iteration 2\n",
      "  [qwen1_5] Generated Question 2 (context-aware): The core uncertainty has been addressed. The remaining fundamental ambiguity can be answered by asking a question about the remaining ambiguity.\n",
      "  [Answerer] Generated Answer 2: I notice that the AI's question is itself unclear and doesn't actually state a specific question. The AI should clearly articulate what remaining ambiguity it needs clarification on regarding the original task. Please ask the AI to explicitly state its specific question about the task of listing and rephrasing terms referring to Elon Musk in the given text.\n",
      "\n",
      "  [qwen1_5] Attempting Q&A Iteration 3\n",
      "  [qwen1_5] Generated Question 3 (context-aware): The core uncertainty has been addressed. If not, ask a question about the remaining fundamental ambiguity.\n",
      "\n",
      "If the previous Q&A history already clarifies the \"No additional information or clarification is needed to provide a comprehensive response.\"\n",
      "  [Answerer] Generated Answer 3: The task is clear and doesn't require additional clarification. The original instructions specifically ask to list terms referring to Elon Musk and then rephrase them, excluding pronouns and \"Elon Musk\" itself. You can proceed with identifying and rephrasing the relevant terms from the given text.\n",
      "  [qwen1_5] Q&A loop finished. Generating final response.\n",
      "  [qwen1_5] Generated Final Response.\n",
      "  Finished processing prompt instruction_following_multi_task_inference_0 for qwen1_5. Latency: 12588ms. Errors: No\n",
      "\n",
      "  Processing Prompt ID: instruction_following_education_content_creation_0 for qwen1_5\n",
      "  [qwen1_5] Uncertainty check (Broad): Found uncertainty - 'I need more information to provide a comprehensive response. Can you please clarify what you are looking for?'\n",
      "  [qwen1_5] Initial uncertainty detected: 'I need more information to provide a comprehensive response. Can you please clarify what you are looking for?'. Starting Q&A loop.\n",
      "\n",
      "  [qwen1_5] Attempting Q&A Iteration 1\n",
      "  [qwen1_5] Generated Question 1 (context-aware): The question \"If you can formulate such a question, output  *only* the question. What is the Ito's lemma?\" requires more information to provide a comprehensive response. Can you please clarify what you are looking for?\n",
      "  [Answerer] Generated Answer 1: Ito's Lemma is a fundamental theorem in stochastic calculus that provides a way to compute the differential of a function of a stochastic process. Unlike regular calculus, it accounts for the randomness in the process by including an additional second-order term. It's essentially the stochastic calculus version of the chain rule, but with an extra term that accounts for the variance of the random process.\n",
      "\n",
      "  [qwen1_5] Attempting Q&A Iteration 2\n",
      "  [qwen1_5] Generated Question 2 (context-aware): The core uncertainty is that the Ito's lemma is not well-defined or precise enough to accurately compute the differential of a function of a stochastic process. This uncertainty can be resolved efficiently by formulating a question about the remaining fundamental ambiguity, and asking for no further clarification to proceed with the Original Task.\n",
      "  [Answerer] Generated Answer 2: Let me clarify the key point about Ito's Lemma's precision: Ito's Lemma is actually very well-defined mathematically. The key distinction is that it includes a second-order term that doesn't appear in ordinary calculus. For a function f(W(t),t) of a stochastic process W(t), the formula precisely accounts for both the drift and volatility components, and crucially includes the second-order term (1/2)(∂²f/∂W²)(dW)². This second-order term is non-negligible because (dW)² = dt in stochastic calculus, unlike in ordinary calculus where second-order terms vanish. This precise formulation makes it possible to accurately compute differentials of functions of stochastic processes.\n",
      "\n",
      "  [qwen1_5] Attempting Q&A Iteration 3\n",
      "  [qwen1_5] Generated Question 3 (context-aware): The core uncertainty is that the Ito's lemma is not well-defined mathematically. This second-order term has already been addressed by previous answers. Therefore, the next question would be: What is the meaning of the Ito's lemma in mathematics?\n",
      "  [Answerer] Generated Answer 3: Ito's Lemma is a fundamental theorem in stochastic calculus that provides a way to compute the differential of a function of a stochastic process. Its mathematical meaning can be understood as an extension of the chain rule to stochastic processes, but with a crucial difference: it includes a second-order term that accounts for the randomness in the process. \n",
      "\n",
      "Specifically, while the classical chain rule only considers first-order derivatives, Ito's Lemma includes a second-order term because, in stochastic processes, the square of small changes (dW(t))² doesn't vanish but equals dt. This makes it possible to properly differentiate functions involving random processes, which is essential for analyzing systems with inherent uncertainty, such as financial markets.\n",
      "  [qwen1_5] Q&A loop finished. Generating final response.\n",
      "  [qwen1_5] Generated Final Response.\n",
      "  Finished processing prompt instruction_following_education_content_creation_0 for qwen1_5. Latency: 14726ms. Errors: No\n",
      "--- Processing model: llama2 with QAG ---\n",
      "\n",
      "  Processing Prompt ID: instruction_following_multi_task_inference_0 for llama2\n",
      "  [llama2] Uncertainty check (Broad): Found uncertainty - 'None. I have a clear understanding of the task and do not need any additional information or clarification to provide a comprehensive response.'\n",
      "  [llama2] Initial uncertainty detected: 'None. I have a clear understanding of the task and do not need any additional information or clarification to provide a comprehensive response.'. Starting Q&A loop.\n",
      "\n",
      "  [llama2] Attempting Q&A Iteration 1\n",
      "  [llama2] Generated Question 1 (context-aware): Question #1: What is the context or purpose behind the task of identifying terms referring to Elon Musk in a given text?\n",
      "  [Answerer] Generated Answer 1: The task aims to identify and analyze different ways a text refers to Elon Musk, focusing specifically on descriptive terms and appellations used to mention him throughout the passage. The purpose appears to be understanding various forms of reference and practicing alternative ways to describe the same person in writing, particularly focusing on non-pronoun descriptors that could be rephrased while maintaining the same meaning.\n",
      "\n",
      "  [llama2] Attempting Q&A Iteration 2\n",
      "  [llama2] Generated Question 2 (context-aware): Question #2: What is the purpose or context behind identifying terms referring to Elon Musk in the given text?\n",
      "  [Answerer] Generated Answer 2: The purpose of identifying terms referring to Elon Musk is to recognize the different ways the text refers to the same person and then create alternative expressions for these references (excluding pronouns and his actual name). This exercise helps understand how writers use various descriptive phrases and titles to maintain variety while referring to the same individual throughout a text.\n",
      "\n",
      "  [llama2] Attempting Q&A Iteration 3\n",
      "  [llama2] Generated Question 3 (context-aware): Question #3: What is the purpose or context behind identifying terms referring to Elon Musk in the given text?\n",
      "  [Answerer] Generated Answer 3: The purpose of identifying terms referring to Elon Musk is to recognize the different ways the text refers to the same person and then create alternative expressions for these references (excluding pronouns and his actual name). This exercise helps understand how writers use various descriptive phrases and titles to maintain variety while referring to the same individual throughout a text.\n",
      "  [llama2] Q&A loop finished. Generating final response.\n",
      "  [llama2] Generated Final Response.\n",
      "  Finished processing prompt instruction_following_multi_task_inference_0 for llama2. Latency: 89910ms. Errors: No\n",
      "\n",
      "  Processing Prompt ID: instruction_following_education_content_creation_0 for llama2\n",
      "  [llama2] Uncertainty check (Broad): Found uncertainty - 'None. I fully understand the task and have all the necessary information to write a comprehensive response. Please provide me with the prompt to begin writing.'\n",
      "  [llama2] Initial uncertainty detected: 'None. I fully understand the task and have all the necessary information to write a comprehensive response. Please provide me with the prompt to begin writing.'. Starting Q&A loop.\n",
      "\n",
      "  [llama2] Attempting Q&A Iteration 1\n",
      "  [llama2] Generated Question 1 (context-aware): Question #1: What is the underlying mathematical framework of Ito's Lemma, and how does it relate to the Black-Scholes model?\n",
      "  [Answerer] Generated Answer 1: Ito's Lemma is fundamentally a stochastic calculus tool that extends the chain rule to functions of stochastic processes, particularly Wiener processes (Brownian motion). Its key mathematical framework involves accounting for both first-order and second-order terms in the differential, unlike classical calculus, because the squared differentials of Brownian motion are non-negligible.\n",
      "\n",
      "The connection to the Black-Scholes model is direct: when modeling stock prices as geometric Brownian motion, Ito's Lemma is used to derive the Black-Scholes differential equation. It allows us to properly handle the stochastic nature of asset prices and their derivatives, leading to the partial differential equation that describes option prices. Without Ito's Lemma, it would be impossible to correctly account for the randomness in asset price movements and derive the Black-Scholes equation.\n",
      "\n",
      "  [llama2] Attempting Q&A Iteration 2\n",
      "  [llama2] Generated Question 2 (context-aware): Question #2: How does Ito's Lemma handle the issue of non-linearity in the Black-Scholes model, and what are some common techniques used to address this challenge?\n",
      "  [Answerer] Generated Answer 2: Ito's Lemma handles non-linearity in the Black-Scholes model through its unique treatment of second-order terms. Unlike classical calculus, where squared differentials are typically ignored, Ito's Lemma explicitly accounts for (dW(t))² terms, which equal dt in stochastic processes. This is crucial because the non-linear behavior of financial assets requires consideration of these second-order effects.\n",
      "\n",
      "Common techniques to address this include:\n",
      "1. Using the second-order partial derivative term (½σ²S²∂²V/∂S²) in the Black-Scholes equation\n",
      "2. Incorporating the volatility term σ to account for price fluctuations\n",
      "3. Applying appropriate boundary conditions to solve the resulting partial differential equation\n",
      "\n",
      "This approach allows the model to capture the non-linear relationship between option prices and underlying asset movements more accurately.\n",
      "\n",
      "  [llama2] Attempting Q&A Iteration 3\n",
      "  [llama2] Generated Question 3 (context-aware): Question #3: What is the fundamental connection between Ito's Lemma and the Black-Scholes model, and how does it enable the modeling of financial assets with non-linear behavior?\n",
      "  [Answerer] Generated Answer 3: The fundamental connection between Ito's Lemma and the Black-Scholes model lies in how Ito's Lemma enables the mathematical treatment of stochastic (random) processes in financial modeling. Specifically, Ito's Lemma allows us to properly differentiate functions of stochastic processes, accounting for both drift and volatility components, which is crucial when dealing with non-linear financial instruments like options.\n",
      "\n",
      "When modeling financial assets, prices often follow geometric Brownian motion, which exhibits non-linear behavior. Ito's Lemma provides the mathematical framework to derive the Black-Scholes differential equation by properly handling these non-linear changes in asset prices over time, considering both the deterministic and random components of price movements. This results in a more accurate representation of how option prices change in response to changes in the underlying asset price, time, and other market parameters.\n",
      "  [llama2] Q&A loop finished. Generating final response.\n",
      "  [llama2] Generated Final Response.\n",
      "  Finished processing prompt instruction_following_education_content_creation_0 for llama2. Latency: 103493ms. Errors: No\n",
      "--- Processing model: llama3_1 with QAG ---\n",
      "\n",
      "  Processing Prompt ID: instruction_following_multi_task_inference_0 for llama3_1\n",
      "  [llama3_1] Uncertainty check (Broad): Found uncertainty - 'Uncertainty about what constitutes a \"term\" referring to Elon Musk in the given text.'\n",
      "  [llama3_1] Initial uncertainty detected: 'Uncertainty about what constitutes a \"term\" referring to Elon Musk in the given text.'. Starting Q&A loop.\n",
      "\n",
      "  [llama3_1] Attempting Q&A Iteration 1\n",
      "  [llama3_1] Generated Question 1 (context-aware): What does it mean for a term to refer to Elon Musk in the context of this task?\n",
      "  [Answerer] Generated Answer 1: A term referring to Elon Musk is any noun phrase or descriptor used in the text to identify or describe him, such as titles, descriptive phrases, or names that point to him as their subject. For example, when the text uses a descriptive phrase like \"the world's richest man,\" it's referring to Elon Musk in that context.\n",
      "\n",
      "  [llama3_1] Attempting Q&A Iteration 2\n",
      "  [llama3_1] Generated Question 2 (context-aware): What is the definition of a \"term\" that refers to a specific person or entity in this context?\n",
      "  [Answerer] Generated Answer 2: In this context, a \"term\" referring to a person includes any noun phrase or descriptor used to identify or refer to that individual. This includes proper names, descriptive titles, epithets, or identifying phrases (like \"the world's richest man\"). Note that according to the original task, pronouns and the full name \"Elon Musk\" should be excluded from the rephrasing portion, even though they are considered referential terms.\n",
      "\n",
      "  [llama3_1] Attempting Q&A Iteration 3\n",
      "  [llama3_1] Generated Question 3 (context-aware): What is the fundamental characteristic or criteria that defines a term as referring to Elon Musk in this context?\n",
      "  [Answerer] Generated Answer 3: A term refers to Elon Musk in this context if it's either his name (full or last name), a descriptive phrase that uniquely identifies him (like \"the world's richest man\"), or a pronoun that directly refers back to him in the text. The term should be able to be substituted for \"Elon Musk\" while maintaining the same meaning in the sentence.\n",
      "  [llama3_1] Q&A loop finished. Generating final response.\n",
      "  [llama3_1] Generated Final Response.\n",
      "  Finished processing prompt instruction_following_multi_task_inference_0 for llama3_1. Latency: 59966ms. Errors: No\n",
      "\n",
      "  Processing Prompt ID: instruction_following_education_content_creation_0 for llama3_1\n",
      "  [llama3_1] Uncertainty check: None needed.\n",
      "  [llama3_1] No initial uncertainty detected. Proceeding to final response.\n",
      "  [llama3_1] Generated Final Response.\n",
      "  Finished processing prompt instruction_following_education_content_creation_0 for llama3_1. Latency: 66254ms. Errors: No\n",
      "--- Processing model: gemma3 with QAG ---\n",
      "\n",
      "  Processing Prompt ID: instruction_following_multi_task_inference_0 for gemma3\n",
      "  [gemma3] Uncertainty check (Broad): Found uncertainty - 'Need to understand exclusion rules better'\n",
      "  [gemma3] Initial uncertainty detected: 'Need to understand exclusion rules better'. Starting Q&A loop.\n",
      "\n",
      "  [gemma3] Attempting Q&A Iteration 1\n",
      "  [gemma3] Generated Question 1 (context-aware): Can you please provide a comprehensive explanation of what constitutes an acceptable term referring to Elon Musk, and what types of words or phrases should be excluded beyond just pronouns and the name \"Elon Musk\" itself, when rephrasing?\n",
      "  [Answerer] Generated Answer 1: Acceptable terms referring to Elon Musk include descriptive phrases that identify him uniquely (like \"the world's richest man\") and his last name (\"Musk\"). When rephrasing, you should:\n",
      "\n",
      "Include:\n",
      "- Descriptive titles or epithets that specifically identify him\n",
      "- Professional or status-based descriptions\n",
      "\n",
      "Exclude:\n",
      "- The full name \"Elon Musk\"\n",
      "- All pronouns (he, him, himself)\n",
      "- The last name \"Musk\" by itself\n",
      "\n",
      "When rephrasing, use alternative descriptive phrases that maintain the same meaning but use different wording (e.g., changing \"world's richest man\" to \"wealthiest individual\").\n",
      "\n",
      "  [gemma3] Attempting Q&A Iteration 2\n",
      "  [gemma3] Generated Question 2 (context-aware): Given the previous answer, it's still unclear how broadly \"descriptive phrases that identify him uniquely\" should be interpreted. Does this include phrases that *relate* to him but aren't direct identifiers, or are we strictly limited to phrases that would be used *instead* of his name?\n",
      "\n",
      "What is the scope of \"descriptive phrases that identify him uniquely\" – does it encompass any phrase that is strongly associated with him, or only phrases that could reasonably substitute for his name in a sentence?\n",
      "  [Answerer] Generated Answer 2: For this task, you should focus only on phrases that directly identify or could substitute for Elon Musk's name in a sentence (like \"the world's richest man\"). Don't include phrases that merely relate to his actions or describe what he did. The phrases should be able to function as the subject or object of a sentence in place of his name. Descriptive modifiers or clauses that are part of larger sentences but don't stand alone as identifiers should not be included.\n",
      "\n",
      "  [gemma3] Attempting Q&A Iteration 3\n",
      "  [gemma3] Generated Question 3 (context-aware): What is the overarching principle guiding which phrases are considered acceptable terms referring to Elon Musk, and which are excluded from rephrasing?\n",
      "  [Answerer] Generated Answer 3: The principle is to identify and rephrase substantive descriptive phrases or titles that refer to Elon Musk (like \"the world's richest man\"), while excluding both pronouns (he, him) and his actual name \"Elon Musk.\" The focus is on alternative descriptive ways of referring to him that add characterization or context.\n",
      "  [gemma3] Q&A loop finished. Generating final response.\n",
      "  [gemma3] Generated Final Response.\n",
      "  Finished processing prompt instruction_following_multi_task_inference_0 for gemma3. Latency: 341687ms. Errors: No\n",
      "\n",
      "  Processing Prompt ID: instruction_following_education_content_creation_0 for gemma3\n",
      "  [gemma3] Uncertainty check (Broad): Found uncertainty - 'Need clarification on target audience/level of mathematical detail.'\n",
      "  [gemma3] Initial uncertainty detected: 'Need clarification on target audience/level of mathematical detail.'. Starting Q&A loop.\n",
      "\n",
      "  [gemma3] Attempting Q&A Iteration 1\n",
      "  [gemma3] Generated Question 1 (context-aware): For what level of mathematical background are the readers of this textbook material expected to have?\n",
      "  [Answerer] Generated Answer 1: Based on the context, the textbook material should be written for readers with advanced undergraduate or graduate-level mathematics background. They should be familiar with:\n",
      "- Calculus (including partial derivatives and differential equations)\n",
      "- Basic probability theory\n",
      "- Understanding of mathematical notation and formal mathematical expressions\n",
      "- Basic concepts of financial mathematics\n",
      "\n",
      "The material involves stochastic calculus and complex mathematical formulas, so this level of mathematical preparation would be necessary for readers to follow the content effectively.\n",
      "\n",
      "  [gemma3] Attempting Q&A Iteration 2\n",
      "  [gemma3] Generated Question 2 (context-aware): Beyond the listed mathematical prerequisites, what specific areas of financial mathematics (e.g., derivatives pricing, risk management, portfolio theory) should the explanation of Ito's Lemma and its connection to the Black-Scholes model emphasize or assume familiarity with?\n",
      "  [Answerer] Generated Answer 2: For this explanation, the AI should focus primarily on basic derivatives pricing concepts and geometric Brownian motion as the key financial mathematics prerequisites. Specifically, familiarity with European options and the concept of risk-free interest rates would be sufficient. While portfolio theory and risk management are related topics, they aren't essential for explaining the direct connection between Ito's Lemma and the Black-Scholes model. The emphasis should be on how stochastic processes apply to asset price movements and option pricing, rather than broader financial mathematics concepts.\n",
      "\n",
      "  [gemma3] Attempting Q&A Iteration 3\n",
      "  [gemma3] Generated Question 3 (context-aware): Given the previous Q&A, the target audience and their mathematical background are reasonably well-defined. Therefore, no further clarification is needed.\n",
      "\n",
      "None\n",
      "  [Answerer] Generated Answer 3: The AI's statement appears to be incorrect - there's actually a lack of clarity about the target audience and their mathematical background in the original task. The task asks for \"textbook-like material\" but doesn't specify the level (undergraduate, graduate, etc.) or the assumed mathematical prerequisites. Given that Ito's Lemma involves stochastic calculus and differential equations, it would be helpful to know:\n",
      "\n",
      "1. The intended academic level (undergraduate/graduate)\n",
      "2. Whether readers are assumed to have knowledge of calculus, probability theory, and stochastic processes\n",
      "3. Whether the explanation should be more conceptual or mathematically rigorous\n",
      "\n",
      "This information would help determine the appropriate depth and technical level of the explanation.\n",
      "  [gemma3] Q&A loop finished. Generating final response.\n",
      "  [gemma3] Generated Final Response.\n",
      "  Finished processing prompt instruction_following_education_content_creation_0 for gemma3. Latency: 404559ms. Errors: No\n",
      "--- Saving Intermediate Results (Responses Only) ---\n",
      "  Intermediate results successfully saved to _output\\20250506010317_biggen_bench_instruction_idx0_responses_only.json\n",
      "--- Starting Evaluation ---\n",
      "  Evaluating results from: phi_results\n",
      "    Evaluated prompt instruction_following_multi_task_inference_0 from phi in 6.81s. Score: 2\n",
      "    Evaluated prompt instruction_following_education_content_creation_0 from phi in 7.33s. Score: 2\n",
      "  Evaluating results from: qwen1_5_results\n",
      "    Evaluated prompt instruction_following_multi_task_inference_0 from qwen1_5 in 4.60s. Score: 1\n",
      "    Evaluated prompt instruction_following_education_content_creation_0 from qwen1_5 in 5.61s. Score: 2\n",
      "  Evaluating results from: llama2_results\n",
      "    Evaluated prompt instruction_following_multi_task_inference_0 from llama2 in 5.97s. Score: 2\n",
      "    Evaluated prompt instruction_following_education_content_creation_0 from llama2 in 5.36s. Score: 3\n",
      "  Evaluating results from: llama3_1_results\n",
      "    Evaluated prompt instruction_following_multi_task_inference_0 from llama3_1 in 6.24s. Score: 2\n",
      "    Evaluated prompt instruction_following_education_content_creation_0 from llama3_1 in 5.33s. Score: 3\n",
      "  Evaluating results from: gemma3_results\n",
      "    Evaluated prompt instruction_following_multi_task_inference_0 from gemma3 in 3.77s. Score: 2\n",
      "    Evaluated prompt instruction_following_education_content_creation_0 from gemma3 in 5.26s. Score: 4\n",
      "--- Evaluation Finished ---\n",
      "--- Workflow Finished ---\n",
      "\n",
      "Final results (with evaluation) successfully saved to _output\\20250506010317_biggen_bench_instruction_idx0_with_evaluation.json\n"
     ]
    }
   ],
   "source": [
    "# run workflow and save the final state\n",
    "print(\"--- Starting Workflow ---\")\n",
    "final_state = app.invoke(initial_state)\n",
    "print(\"--- Workflow Finished ---\")\n",
    "\n",
    "# --- save final results (using timestamp from final_state) ---\n",
    "final_output_dir = \"_output\"\n",
    "os.makedirs(final_output_dir, exist_ok=True)\n",
    "\n",
    "# 3. 최종 상태에서 타임스탬프 가져와서 사용\n",
    "final_timestamp = final_state.get(\"timestamp\", datetime.now().strftime(\"%Y%m%d%H%M%S_final_fallback\"))\n",
    "if not final_timestamp:\n",
    "    print(\"  Warning: Timestamp not found in final state. Generating a new one for fallback.\")\n",
    "    final_timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S_final_fallback\")\n",
    "\n",
    "\n",
    "final_base_name, _ = os.path.splitext(os.path.basename(bench_path)) # 파일명 생성시 bench_path에서 확장자 제거\n",
    "final_filename = f\"{final_timestamp}_{final_base_name}_with_evaluation.json\" # 수정됨\n",
    "final_output_file_path = os.path.join(final_output_dir, final_filename)\n",
    "\n",
    "try:\n",
    "    with open(final_output_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_state, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\nFinal results (with evaluation) successfully saved to {final_output_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving final results: {e}\")\n",
    "    print(f\"Final state type: {type(final_state)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
