{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "\n",
    "o3_mini = init_chat_model(\"openai:o3-mini\")\n",
    "claude_sonnet = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0)\n",
    "gemma3 = init_chat_model(\"ollama:gemma3:12b\", temperature=0)\n",
    "llama3_1 = init_chat_model(\"ollama:llama3.1:latest\", temperature=0)\n",
    "deepseek_r1 = init_chat_model(\"ollama:deepseek-r1:8b\", temperature=0)\n",
    "evaluator = init_chat_model(\"anthropic:claude-3-7-sonnet-20250219\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing benchmark data\n",
    "\n",
    "bench_path = \"biggen_bench_instruction_idx0.json\"\n",
    "\n",
    "def load_benchmark_data(bench_path: str) -> List[Dict]:\n",
    "    \"\"\"Load benchmark data from a JSON file.\"\"\"\n",
    "    with open(bench_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "benchmark_data = load_benchmark_data(bench_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_prompts(benchmark_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Prepare prompts by excluding reference_answer and score_rubric.\"\"\"\n",
    "    prompts = []\n",
    "    for item in benchmark_data:\n",
    "        prompt = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"capability\": item[\"capability\"],\n",
    "            \"task\": item[\"task\"],\n",
    "            \"instance_idx\": item[\"instance_idx\"],\n",
    "            \"system_prompt\": item[\"system_prompt\"],\n",
    "            \"input\": item[\"input\"],\n",
    "            # Exclude reference_answer and score_rubric\n",
    "        }\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "def prepare_rubric(benchmark_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Prepare rubric including reference_answer and score_rubric.\"\"\"\n",
    "    rubric = []\n",
    "    for item in benchmark_data:\n",
    "        prompt = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"reference_answer\": item[\"reference_answer\"],\n",
    "            \"score_rubric\": item[\"score_rubric\"]\n",
    "        }\n",
    "        rubric.append(prompt)\n",
    "    return rubric\n",
    "\n",
    "prompts = prepare_prompts(benchmark_data)\n",
    "rubric = prepare_rubric(benchmark_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state structure\n",
    "class State(TypedDict):\n",
    "    prompts: List[Dict[str, Any]]\n",
    "    processed_count: int\n",
    "    gemma3_results: List[Dict[str, Any]]\n",
    "    llama3_1_results: List[Dict[str, Any]]\n",
    "    deepseek_r1_results: List[Dict[str, Any]]\n",
    "    evaluator_results: List[Dict[str, Any]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_processor(model, model_name):\n",
    "    def process_model(state: State) -> State:\n",
    "        results = []\n",
    "        \n",
    "        for prompt in state[\"prompts\"]:\n",
    "            system_prompt = prompt.get('system_prompt', '')\n",
    "            user_input = prompt.get('input', '')\n",
    "            \n",
    "            try:\n",
    "                response = model.invoke(\n",
    "                    user_input,\n",
    "                    config={\"system_prompt\": system_prompt}\n",
    "                )\n",
    "                \n",
    "                response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "                result = {\n",
    "                    \"id\": prompt.get('id', ''),\n",
    "                    \"model_name\": model_name,\n",
    "                    \"response\": response_content\n",
    "                }\n",
    "            except Exception as e:\n",
    "                result = {\n",
    "                    \"id\": prompt.get('id', ''),\n",
    "                    \"model_name\": model_name,\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "                \n",
    "            results.append(result)\n",
    "        \n",
    "        results_key = f\"{model_name}_results\"\n",
    "        new_state = state.copy()\n",
    "        new_state[results_key] = results\n",
    "        return new_state\n",
    "    \n",
    "    return process_model\n",
    "\n",
    "# create model processor variables\n",
    "process_gemma3 = create_model_processor(gemma3, \"gemma3\")\n",
    "process_llama3_1 = create_model_processor(llama3_1, \"llama3_1\")\n",
    "# process_deepseek_r1 = create_model_processor(deepseek_r1, \"deepseek_r1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial state setting\n",
    "initial_state = {\n",
    "    \"prompts\": prompts,\n",
    "    \"processed_count\": 0,\n",
    "    \"gemma3_results\": [],\n",
    "    \"llama3_1_results\": [],\n",
    "#    \"deepseek_r1_results\": []\n",
    "}\n",
    "\n",
    "# create workflow\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"process_gemma3\", process_gemma3)\n",
    "workflow.add_node(\"process_llama3\", process_llama3_1)\n",
    "#workflow.add_node(\"process_deepseek\", process_deepseek_r1)\n",
    "\n",
    "# connect nodes\n",
    "workflow.set_entry_point(\"process_gemma3\")\n",
    "workflow.add_edge(\"process_gemma3\", \"process_llama3\")\n",
    "#workflow.add_edge(\"process_llama3\", \"process_deepseek\")\n",
    "\n",
    "# compile workflow\n",
    "app = workflow.compile()\n",
    "\n",
    "# run workflow\n",
    "final_state = app.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과가 _output/20250505131906_biggen_bench_instruction_idx0.json_response.json 에 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# check final_state whether dict \n",
    "if isinstance(final_state, dict):\n",
    "    # save final_state as JSON file\n",
    "    output_file_path = f\"_output/{timestamp}_{bench_path}_response.json\"\n",
    "    try:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_state, f, ensure_ascii=False, indent=4) # ensure_ascii=False는 한글 깨짐 방지, indent는 가독성 향상\n",
    "        print(f\"결과가 {output_file_path} 에 성공적으로 저장되었습니다.\")\n",
    "    except TypeError as e:\n",
    "        print(f\"JSON 직렬화 오류: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"오류: 최종 결과의 타입이 dict가 아닙니다. 타입: {type(final_state)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_path = f\"_output/{timestamp}_{bench_path}_response.json\"\n",
    "\n",
    "def load_benchmark_data(response_path: str) -> List[Dict]:\n",
    "    \"\"\"Load benchmark data from a JSON file.\"\"\"\n",
    "    with open(response_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "response_data = load_benchmark_data(response_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ChatAnthropic' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     19\u001b[39m                     evaluations.append({\n\u001b[32m     20\u001b[39m                         \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: prompt_id,\n\u001b[32m     21\u001b[39m                         \u001b[33m\"\u001b[39m\u001b[33magent_name\u001b[39m\u001b[33m\"\u001b[39m: response[\u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     22\u001b[39m                         \u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m: evaluation[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     23\u001b[39m                         \u001b[33m\"\u001b[39m\u001b[33mrationale\u001b[39m\u001b[33m\"\u001b[39m: evaluation[\u001b[33m\"\u001b[39m\u001b[33mrationale\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     24\u001b[39m                     })\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluations\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m evaluations = \u001b[43mevaluate_responses_with_langgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbenchmark_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m evaluation \u001b[38;5;129;01min\u001b[39;00m evaluations:\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(evaluation)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mevaluate_responses_with_langgraph\u001b[39m\u001b[34m(response_data, benchmark_data, evaluator)\u001b[39m\n\u001b[32m     12\u001b[39m             benchmark_item = benchmark_map.get(prompt_id)\n\u001b[32m     13\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m benchmark_item:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m                 evaluation = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m(\n\u001b[32m     15\u001b[39m                     response=response[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     16\u001b[39m                     reference=benchmark_item[\u001b[33m\"\u001b[39m\u001b[33mreference_answer\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     17\u001b[39m                     rubric=benchmark_item[\u001b[33m\"\u001b[39m\u001b[33mscore_rubric\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     18\u001b[39m                 )\n\u001b[32m     19\u001b[39m                 evaluations.append({\n\u001b[32m     20\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: prompt_id,\n\u001b[32m     21\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33magent_name\u001b[39m\u001b[33m\"\u001b[39m: response[\u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     22\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m: evaluation[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     23\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mrationale\u001b[39m\u001b[33m\"\u001b[39m: evaluation[\u001b[33m\"\u001b[39m\u001b[33mrationale\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     24\u001b[39m                 })\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m evaluations\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\code_multiLLM_agent_toy\\myvenv\\Lib\\site-packages\\pydantic\\main.py:994\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    991\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    993\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m994\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'ChatAnthropic' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "def evaluate_responses_with_langgraph(response_data: Dict, benchmark_data: List[Dict], evaluator) -> List[Dict]:\n",
    "    \"\"\"Evaluate responses using the langgraph Evaluator model and return scores.\"\"\"\n",
    "    \n",
    "    benchmark_map = {item[\"id\"]: item for item in benchmark_data}\n",
    "    evaluations = []\n",
    "    \n",
    "    # gemma3_results와 llama3_1_results에서 응답 평가\n",
    "    for model_results_key in ['gemma3_results', 'llama3_1_results']:\n",
    "        if model_results_key in response_data:\n",
    "            for response in response_data[model_results_key]:\n",
    "                prompt_id = response[\"id\"]\n",
    "                benchmark_item = benchmark_map.get(prompt_id)\n",
    "                if benchmark_item:\n",
    "                    evaluation = evaluator.evaluate(\n",
    "                        response=response[\"response\"],\n",
    "                        reference=benchmark_item[\"reference_answer\"],\n",
    "                        rubric=benchmark_item[\"score_rubric\"]\n",
    "                    )\n",
    "                    evaluations.append({\n",
    "                        \"id\": prompt_id,\n",
    "                        \"agent_name\": response[\"model_name\"],\n",
    "                        \"score\": evaluation[\"score\"],\n",
    "                        \"rationale\": evaluation[\"rationale\"]\n",
    "                    })\n",
    "    \n",
    "    return evaluations\n",
    "\n",
    "evaluations = evaluate_responses_with_langgraph(response_data, benchmark_data, evaluator)\n",
    "\n",
    "for evaluation in evaluations:\n",
    "    print(evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
