{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chat models\n",
    "o3_mini = init_chat_model(\"openai:o3-mini\")\n",
    "claude_haiku = init_chat_model(\"anthropic:claude-3-haiku-20240307\", temperature=0)\n",
    "phi = init_chat_model(\"ollama:phi:latest\", temperature=0)\n",
    "qwen1_5 = init_chat_model(\"ollama:qwen:0.5b\", temperature=0)\n",
    "vicuna = init_chat_model(\"ollama:vicuna:7b\", temperature=0)\n",
    "llama2 = init_chat_model(\"ollama:llama2:latest\", temperature=0)\n",
    "llama3_1 = init_chat_model(\"ollama:llama3.1:latest\", temperature=0)\n",
    "deepseek_r1 = init_chat_model(\"ollama:deepseek-r1:8b\", temperature=0)\n",
    "gemma3 = init_chat_model(\"ollama:gemma3:12b\", temperature=0)\n",
    "answerer = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0)\n",
    "evaluator = init_chat_model(\"openai:gpt-4-turbo-2024-04-09\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing benchmark data and prompts, rubric\n",
    "\n",
    "bench_path = \"biggen_bench_test_4instance.json\"\n",
    "\n",
    "def load_benchmark_data(bench_path: str) -> List[Dict]:\n",
    "    \"\"\"Load benchmark data from a JSON file.\"\"\"\n",
    "    with open(bench_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "benchmark_data = load_benchmark_data(bench_path)\n",
    "\n",
    "def prepare_prompts(benchmark_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Prepare prompts by excluding reference_answer and score_rubric.\"\"\"\n",
    "    prompts = []\n",
    "    for item in benchmark_data:\n",
    "        prompt = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"capability\": item[\"capability\"],\n",
    "            \"task\": item[\"task\"],\n",
    "            \"instance_idx\": item[\"instance_idx\"],\n",
    "            \"system_prompt\": item[\"system_prompt\"],\n",
    "            \"input\": item[\"input\"],\n",
    "            # Exclude reference_answer and score_rubric\n",
    "        }\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "def prepare_rubric(benchmark_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Prepare rubric including reference_answer and score_rubric.\"\"\"\n",
    "    rubric = []\n",
    "    for item in benchmark_data:\n",
    "        prompt = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"reference_answer\": item[\"reference_answer\"],\n",
    "            \"score_rubric\": item[\"score_rubric\"]\n",
    "        }\n",
    "        rubric.append(prompt)\n",
    "    return rubric\n",
    "\n",
    "prompts = prepare_prompts(benchmark_data)\n",
    "rubrics = prepare_rubric(benchmark_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state structure\n",
    "class State(TypedDict):\n",
    "    prompts: List[Dict[str, Any]]\n",
    "    rubrics: Dict[str, Dict[str, Any]] \n",
    "#    processed_count: int\n",
    "    phi_results: List[Dict[str, Any]]\n",
    "    qwen1_5_results: List[Dict[str, Any]]\n",
    "    vicuna_results: List[Dict[str, Any]]\n",
    "    llama2_results: List[Dict[str, Any]]\n",
    "    llama3_1_results: List[Dict[str, Any]]\n",
    "    gemma3_results: List[Dict[str, Any]]\n",
    "#    deepseek_r1_results: List[Dict[str, Any]]\n",
    "    evaluation_results: List[Dict[str, Any]]\n",
    "    timestamp: str  # 워크플로우 전체에서 공유할 타임스탬프 필드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuestionAugmentedProcessor Class to replace the create_model_processor function\n",
    "class QuestionAugmentedProcessor:\n",
    "    def __init__(self, model, model_name, answerer, benchmark_data):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.answerer = answerer\n",
    "        # Create a mapping from prompt ID to benchmark item for easy lookup\n",
    "        self.benchmark_map = {item[\"id\"]: item for item in benchmark_data}\n",
    "        print(f\"Initialized QuestionAugmentedProcessor for {self.model_name} with {len(self.benchmark_map)} benchmark items.\")\n",
    "\n",
    "    def _extract_content(self, response: Any) -> str:\n",
    "        \"\"\"Helper function to reliably extract string content from model responses.\"\"\"\n",
    "        if isinstance(response, BaseMessage) and hasattr(response, 'content'):\n",
    "            return response.content.strip()\n",
    "        elif isinstance(response, str):\n",
    "            return response.strip()\n",
    "        elif isinstance(response, dict) and 'content' in response:\n",
    "            return response['content'].strip()\n",
    "        elif hasattr(response, 'text'): # Handle potential other response objects\n",
    "             return response.text.strip()\n",
    "        else:\n",
    "            # Fallback: attempt to convert to string, might not be ideal\n",
    "            print(f\"Warning: Unexpected response type {type(response)}. Attempting string conversion.\")\n",
    "            return str(response).strip()\n",
    "\n",
    "    def generate_uncertainty_check(self, system_prompt: str, user_input: str) -> str | None:\n",
    "        \"\"\"Check if the model needs clarification before answering.\"\"\"\n",
    "        uncertainty_prompt_text = f\"\"\"\n",
    "        Review the task: \"{user_input}\"\n",
    "\n",
    "        Determine if you need any additional information or clarification to provide a comprehensive response.\n",
    "        - If NO: Respond with the single word \"None\". MUST BE ONLY \"None\".\n",
    "        - If YES: Briefly state the main area of uncertainty (max 10 words). Example: \"Need scope clarification\". Output ONLY the uncertainty phrase.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            uncertainty_response = self.model.invoke(\n",
    "                uncertainty_prompt_text,\n",
    "                config={\"system_prompt\": system_prompt} if system_prompt else None\n",
    "            )\n",
    "            uncertainty_text = self._extract_content(uncertainty_response)\n",
    "            cleaned_uncertainty = uncertainty_text.strip().strip('.').lower()\n",
    "\n",
    "            # Clean up and standardize the response\n",
    "            # Strict check for \"None\"\n",
    "            if cleaned_uncertainty == \"none\":\n",
    "                 print(f\"  [{self.model_name}] Uncertainty check: Strictly 'None'.\")\n",
    "                 return None # Return Python None\n",
    "            elif not cleaned_uncertainty:\n",
    "                 print(f\"  [{self.model_name}] Uncertainty check: Empty response. Assuming None.\")\n",
    "                 return None\n",
    "            else:\n",
    "                 # Return the original (un-lowercased) response if it's not \"None\"\n",
    "                 print(f\"  [{self.model_name}] Uncertainty check: Found uncertainty - '{uncertainty_text}'\")\n",
    "                 # Return the potentially longer phrase if model didn't follow max words, but it's not None\n",
    "                 return uncertainty_text.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  [{self.model_name}] Error checking uncertainty: {e}\")\n",
    "            return None # Return None on error\n",
    "\n",
    "\n",
    "    def generate_question(self, system_prompt: str, user_input: str, uncertainty: str, question_num: int, inquiring_history: List[Dict]) -> str | None:\n",
    "        \"\"\"Generate a clarifying question based on uncertainty, considering previous Q&A.\"\"\"\n",
    "        # Format the history for the prompt\n",
    "        formatted_history = \"None\" \n",
    "        if inquiring_history and isinstance(inquiring_history[0], dict):\n",
    "            first_entry = inquiring_history[0]\n",
    "            prev_q = first_entry.get(\"question_0\")\n",
    "            prev_a = first_entry.get(\"answer_0\")\n",
    "\n",
    "            if prev_q and prev_a: # If both question and answer exist\n",
    "                formatted_history = f\"previous question: {prev_q}\\nprevious answer: {prev_a}\"\n",
    "            elif prev_q: # If only question exists (typically not the case)\n",
    "                 formatted_history = f\"previous question: {prev_q}\\n(no answer yet)\"\n",
    "            # If only uncertainty is recorded, formatted_history remains the default value\n",
    "\n",
    "        # prompt with history and instructions\n",
    "        question_prompt_text = f\"\"\"\n",
    "        Task: \"{user_input}\"\n",
    "        Detected Uncertainty: \"{uncertainty}\"\n",
    "\n",
    "        Based on the Uncertainty about the Task, generate a single clarifying question.\n",
    "        Output ONLY the clear, and concise question.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            question_response = self.model.invoke(\n",
    "                question_prompt_text\n",
    "            )\n",
    "            question = self._extract_content(question_response)\n",
    "\n",
    "            # Check if the model indicates no more questions are needed\n",
    "            cleaned_question = question.strip().strip('.').strip()\n",
    "            if cleaned_question.lower() == \"none\":\n",
    "                print(f\"  [{self.model_name}] Model indicates no further questions needed.\")\n",
    "                return None # Signal to stop asking questions\n",
    "            elif not cleaned_question:\n",
    "                print(f\"  [{self.model_name}] returning None due to empty question.\")\n",
    "                return None # reagerding empty to be None\n",
    "            else:\n",
    "                print(f\"  [{self.model_name}] Generated Question {question_num+1} (context-aware): {question}\")\n",
    "                return question # Return the new question\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  [{self.model_name}] Error generating context-aware question {question_num+1}: {e}\")\n",
    "            # Return None on error to prevent infinite loops or error cascades\n",
    "            return None # Signal to stop on error as well\n",
    "\n",
    "    def generate_answer(self, prompt_id: str, question: str, user_input: str, question_num: int) -> str:\n",
    "        \"\"\"Generate an answer to a clarifying question using the answerer model.\"\"\"\n",
    "        benchmark_item = self.benchmark_map.get(prompt_id)\n",
    "        if not benchmark_item:\n",
    "            print(f\"  [Answerer] Error: Benchmark data not found for prompt ID {prompt_id}.\")\n",
    "            return \"Error: Could not find reference data.\"\n",
    "\n",
    "        reference_answer = benchmark_item.get(\"reference_answer\", \"No reference answer available.\")\n",
    "\n",
    "        # More robust answerer prompt\n",
    "        answer_prompt_text = f\"\"\"\n",
    "        You are an assistant providing clarification to another AI.\n",
    "        The original task given to the AI was:\n",
    "        Original Task: {user_input}\n",
    "        Another AI's question: {question}\n",
    "\n",
    "        Provide a *brief* and *focused* answer to the AI's specific question.\n",
    "        Use the context from the 'Original Task' and the 'Reference Answer' \n",
    "        Reference Answer (Context Only - Do Not Reveal Directly): \n",
    "        {reference_answer}\n",
    "\n",
    "        - *Do not* solve the original task. \n",
    "        - *Do not* reveal the reference answer directly.\n",
    "        - Just provide the information needed to answer the AI's specific question concisely.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Use the dedicated answerer model\n",
    "            answer_response = self.answerer.invoke(answer_prompt_text)\n",
    "            answer = self._extract_content(answer_response)\n",
    "            print(f\"  [Answerer] Generated Answer {question_num+1}: {answer}\")\n",
    "            return answer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  [Answerer] Error generating answer for question {question_num+1}: {e}\")\n",
    "            return f\"Error generating answer to question {question_num + 1}.\"\n",
    "\n",
    "    def generate_final_response(self, system_prompt: str, user_input: str, inquiring_list: List[Dict]) -> str:\n",
    "        \"\"\"Generate the final response based on original input and answers in inquiring Q&A\"\"\"\n",
    "\n",
    "        context_section = \"\\nNo clarification was requested or provided.\\n\"\n",
    "\n",
    "        # check if inquiring_list has items and if the first item is a dictionary\n",
    "        if inquiring_list and isinstance(inquiring_list[0], dict):\n",
    "            first_entry = inquiring_list[0]\n",
    "            clarification_answer = first_entry.get(\"answer_0\") # check if answer_0 exists\n",
    "\n",
    "            if clarification_answer:\n",
    "                # if clarification_answer exists, add it to the context_section\n",
    "                context_section = \"\\n--- Clarifications Provided During Process ---\\n{clarification_answer}\\n--- 추가 정보 끝 ---\\n\"\n",
    "            # in case initial uncertainty is detected but model decided not to ask a question ('None' from generate_question)\n",
    "            elif first_entry.get(\"uncertainty\") is not None and first_entry.get(\"question_0\") is None:\n",
    "                 context_section = \"\\n--- initial uncertainty was detected but model decided not to ask a question. ---\\n\"\n",
    "            # in case initial uncertainty is not detected\n",
    "            elif first_entry.get(\"uncertainty\") is None:\n",
    "                 context_section = \"\\n--- No additional question was needed ---\\n\"\n",
    "            # other cases (e.g., question was generated but answer generation failed) -> keep the default value or handle separately\n",
    "\n",
    "        final_prompt_text = f\"\"\"\n",
    "        You are tasked with completing the following request:\n",
    "        Original Request: {user_input}\n",
    "\n",
    "        Additional clarifications provided during the process (if any):\n",
    "        {context_section}\n",
    "\n",
    "        Based *only* on the Original Request and the Clarifications Provided above, generate the complete and final response.\n",
    "        Adhere strictly to the requirements of the Original Request.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            final_response_obj = self.model.invoke(\n",
    "                final_prompt_text,\n",
    "                config={\"system_prompt\": system_prompt} if system_prompt else None\n",
    "            )\n",
    "            final_response = self._extract_content(final_response_obj)\n",
    "            print(f\"  [{self.model_name}] Generated Final Response (using answers as context).\")\n",
    "            return final_response\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  [{self.model_name}] Error generating final response: {e}\")\n",
    "            return \"Error generating final response.\"\n",
    "\n",
    "    def process(self, state: State) -> Dict[str, Any]:\n",
    "        \"\"\"Main processor method that handles the entire QAG workflow for this model.\"\"\"\n",
    "        results = []\n",
    "        model_results_key = f\"{self.model_name}_results\"\n",
    "        print(f\"--- Processing model: {self.model_name} with QAG ---\")\n",
    "\n",
    "        prompts_to_process = state.get(\"prompts\", [])\n",
    "        if not prompts_to_process:\n",
    "            print(f\"  No prompts found for {self.model_name}.\")\n",
    "            return {model_results_key: []} # Return empty list for this model\n",
    "\n",
    "        for prompt in prompts_to_process:\n",
    "            prompt_id = prompt.get('id', f'unknown_id_{time.time()}')\n",
    "            system_prompt = prompt.get('system_prompt', '')\n",
    "            user_input = prompt.get('input', '')\n",
    "\n",
    "            print(f\"\\n  Processing Prompt ID: {prompt_id} for {self.model_name}\")\n",
    "\n",
    "            if not user_input:\n",
    "                print(f\"  Skipping prompt {prompt_id} for {self.model_name} due to empty input.\")\n",
    "                result = {\n",
    "                    \"id\": prompt_id,\n",
    "                    \"model_name\": self.model_name,\n",
    "                    \"inquiring\": [], # Keep structure consistent\n",
    "                    \"response\": None,\n",
    "                    \"error\": \"Skipped due to empty input\",\n",
    "                    \"latency\": 0\n",
    "                }\n",
    "                results.append(result)\n",
    "                continue\n",
    "\n",
    "            start_time = time.time()\n",
    "            inquiring_list = [] # Initialize list to store Q&A history *for this prompt*\n",
    "            final_response = None\n",
    "            error_message = None\n",
    "\n",
    "            try:\n",
    "                # 1. Initial Uncertainty Check (Done only ONCE)\n",
    "                initial_uncertainty = self.generate_uncertainty_check(system_prompt, user_input)\n",
    "\n",
    "                if initial_uncertainty is None:\n",
    "                     # Case: No uncertainty, generate response directly\n",
    "                     print(f\"  [{self.model_name}] No initial uncertainty detected. Proceeding to final response.\")\n",
    "                     inquiring_list.append({\"uncertainty\": None}) # Keep minimal entry\n",
    "                     final_response = self.generate_final_response(system_prompt, user_input, [])\n",
    "\n",
    "                else:\n",
    "                    # Case: Uncertainty detected, make a single question\n",
    "                    print(f\"  [{self.model_name}] Initial uncertainty detected: '{initial_uncertainty}'. Asking a question.\")\n",
    "                    max_questions = 3\n",
    "\n",
    "                    question_count = 0\n",
    "                    print(f\"\\n  [{self.model_name}] Resolving uncertainty via Q&A {question_count + 1}\")\n",
    "\n",
    "                        # Generate Question based on the detected uncertainty\n",
    "                    question = self.generate_question(\n",
    "                        system_prompt,\n",
    "                        user_input,\n",
    "                        initial_uncertainty, # Pass the initially identified uncertainty\n",
    "                        question_count,\n",
    "                        [] \n",
    "                        )\n",
    "\n",
    "                    # Check if generate_question returned None (meaning stop)\n",
    "                    if question is None:\n",
    "                        print(f\"  [{self.model_name}] Generated Question {question_count+1}: {question}\")\n",
    "                        inquiry_step = {\n",
    "                            \"uncertainty\": initial_uncertainty, # 해당 라운드를 유발한 초기 불확실성 기록\n",
    "                            f\"question_{question_count}\": question\n",
    "                        }\n",
    "\n",
    "                        # Answer genertation\n",
    "                        answer = self.generate_answer(prompt_id, question, user_input, question_count)\n",
    "                        inquiry_step[f\"answer_{question_count}\"] = answer\n",
    "                        inquiring_list.append(inquiry_step)\n",
    "\n",
    "                    else: # If no uncertainty was detected, generate final response directly\n",
    "                        print(f\"  [{self.model_name}] Model indicated no question needed. Generating final response.\")\n",
    "                        inquiring_list.append({\"uncertainty\": initial_uncertainty, f\"question_{question_count}\": None, f\"answer_{question_count}\": None})\n",
    "\n",
    "\n",
    "                # After clarifying Q&A attempt, generate final response immediately and pass the Q&A history\n",
    "                print(f\"  [{self.model_name}] Generating final response after clarifying Q&A attempt.\")\n",
    "                final_response = self.generate_final_response(system_prompt, user_input, inquiring_list)\n",
    "\n",
    "            # Error handling\n",
    "            except Exception as e:\n",
    "                 error_message = f\"Unhandled error during processing prompt {prompt_id} for {self.model_name}: {type(e).__name__}: {e}\"\n",
    "                 print(f\"  {error_message}\")\n",
    "                 if not inquiring_list: inquiring_list = [] # Ensure it's a list\n",
    "\n",
    "            # latency calculation \n",
    "            end_time = time.time()\n",
    "            latency = int((end_time - start_time) * 1000)\n",
    "\n",
    "            # Assemble final result object for this prompt\n",
    "            result = {\n",
    "                \"id\": prompt_id,\n",
    "                \"model_name\": self.model_name,\n",
    "                \"inquiring\": inquiring_list,\n",
    "                \"response\": final_response,\n",
    "                \"latency\": latency,\n",
    "                \"error\": error_message\n",
    "            }\n",
    "            results.append(result)\n",
    "            print(f\"  Finished processing prompt {prompt_id} for {self.model_name}. Latency: {latency}ms. Errors: {'Yes' if error_message else 'No'}\")\n",
    "\n",
    "        # Return results for this model\n",
    "        return {model_results_key: results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processors using the new class\n",
    "# Pass the benchmark_data loaded earlier (ensure it's available in this scope)\n",
    "phi_processor = QuestionAugmentedProcessor(phi, \"phi\", answerer, benchmark_data)\n",
    "qwen1_5_processor = QuestionAugmentedProcessor(qwen1_5, \"qwen1_5\", answerer, benchmark_data)\n",
    "vicuna_processor = QuestionAugmentedProcessor(vicuna, \"vicuna\", answerer, benchmark_data)\n",
    "llama2_processor = QuestionAugmentedProcessor(llama2, \"llama2\", answerer, benchmark_data)\n",
    "llama3_1_processor = QuestionAugmentedProcessor(llama3_1, \"llama3_1\", answerer, benchmark_data)\n",
    "gemma3_processor = QuestionAugmentedProcessor(gemma3, \"gemma3\", answerer, benchmark_data)   \n",
    "# process_deepseek_r1 = QuestionAugmentedProcessor(deepseek_r1, \"deepseek_r1\", answerer, benchmark_data) # Keep commented if not used\n",
    "\n",
    "# Define the process functions that will be used as nodes\n",
    "def process_phi(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"Node function to run the phi processor.\"\"\"\n",
    "    return phi_processor.process(state)\n",
    "\n",
    "def process_qwen1_5(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"Node function to run the qwen1_5 processor.\"\"\"\n",
    "    return qwen1_5_processor.process(state)\n",
    "\n",
    "def process_vicuna(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"Node function to run the vicuna processor.\"\"\"\n",
    "    return vicuna_processor.process(state)\n",
    "\n",
    "def process_llama2(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"Node function to run the llama2 processor.\"\"\"\n",
    "    return llama2_processor.process(state)\n",
    "\n",
    "def process_llama3_1(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"Node function to run the llama3_1 processor.\"\"\"\n",
    "    return llama3_1_processor.process(state)\n",
    "\n",
    "def process_gemma3(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"Node function to run the gemma3 processor.\"\"\"\n",
    "    return gemma3_processor.process(state)\n",
    "\n",
    "# Define other nodes (save_intermediate_results, evaluate_responses)\n",
    "# Ensure these functions are defined as they were in your original notebook (Cells 7 and 8)\n",
    "# ... (Make sure the save_intermediate_results and evaluate_responses functions are defined here or previously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- define save intermediate results node ---\n",
    "def save_intermediate_results(state: State) -> Dict:\n",
    "    \"\"\"Saves the generated responses before evaluation using the timestamp from the state.\"\"\"\n",
    "    print(\"--- Saving Intermediate Results (Responses Only) ---\")\n",
    "    # State로부터 타임스탬프 가져오기\n",
    "    timestamp = state.get(\"timestamp\", datetime.now().strftime(\"%Y%m%d%H%M%S_fallback\"))\n",
    "    if not timestamp:\n",
    "        print(\"  Warning: Timestamp not found in state. Generating a new one for fallback.\")\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S_fallback\")\n",
    "    \n",
    "    output_dir = \"_output\"\n",
    "    os.makedirs(output_dir, exist_ok=True) # 출력 디렉토리 확인 및 생성\n",
    "\n",
    "    # 저장할 데이터 구성 (응답 결과만 선택)\n",
    "    data_to_save = {}\n",
    "    model_result_keys = [key for key in state if key.endswith(\"_results\") and key not in [\"evaluation_results\"]]\n",
    "    for key in model_result_keys:\n",
    "        if state.get(key):\n",
    "             data_to_save[key] = state[key]\n",
    "\n",
    "    # 프롬프트 정보도 함께 저장하고 싶다면 추가\n",
    "    # data_to_save[\"prompts\"] = state.get(\"prompts\", [])\n",
    "\n",
    "    if not data_to_save:\n",
    "        print(\"  No response data found to save.\")\n",
    "        return {} # 저장할 데이터 없으면 아무것도 안함\n",
    "\n",
    "    base_name, _ = os.path.splitext(os.path.basename(bench_path)) # 파일명 생성시 bench_path에서 확장자 제거\n",
    "    intermediate_filename = f\"{timestamp}_{base_name}_responses_only.json\" # 수정됨\n",
    "    output_file_path = os.path.join(output_dir, intermediate_filename)\n",
    "\n",
    "    try:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data_to_save, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"  Intermediate results successfully saved to {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error saving intermediate results: {e}\")\n",
    "\n",
    "    # 이 노드는 상태를 변경하지 않으므로 빈 딕셔셔리 반환\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Evaluation node ---\n",
    "\n",
    "# 평가 프롬프트 템플릿 정의 (biggen_bench 구조 기반)\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert evaluator. Your task is to evaluate an AI assistant's response based on the provided user query, reference answer, and a detailed scoring rubric.\n",
    "Focus ONLY on the provided information and rubric. Assign a score from 1 to 5, where 5 is the best, according to the descriptions.\n",
    "Provide your output strictly in the specified format.\"\"\"),\n",
    "    (\"human\", \"\"\"\n",
    "**Evaluation Context:**\n",
    "\n",
    "* **Task Type:** {task_description}\n",
    "* **User Query:**\n",
    "    ```\n",
    "    {user_query}\n",
    "    ```\n",
    "* **Reference Answer:**\n",
    "    ```\n",
    "    {reference_answer}\n",
    "    ```\n",
    "* **AI Response to Evaluate:**\n",
    "    ```\n",
    "    {ai_response}\n",
    "    ```\n",
    "\n",
    "**Scoring Rubric:**\n",
    "\n",
    "* **Criteria:** {criteria}\n",
    "* **Score 1 Description:** {score1_desc}\n",
    "* **Score 2 Description:** {score2_desc}\n",
    "* **Score 3 Description:** {score3_desc}\n",
    "* **Score 4 Description:** {score4_desc}\n",
    "* **Score 5 Description:** {score5_desc}\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  Carefully compare the \"AI Response to Evaluate\" against the \"Reference Answer\" and the \"Scoring Rubric\".\n",
    "2.  Determine the score (1-5) that best reflects the quality of the AI Response according to the rubric descriptions.\n",
    "3.  Provide a brief rationale explaining *why* you chose that score, referencing specific aspects of the rubric descriptions and the AI response.\n",
    "\n",
    "**Output Format (MUST follow exactly):**\n",
    "Score: [Your score between 1-5]\n",
    "Rationale: [Your concise explanation based on the rubric]\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "# --- evaluate_responses 함수 내에서 이 템플릿을 사용하는 방법 ---\n",
    "\n",
    "def evaluate_responses(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluates responses from different models based on rubrics.\"\"\"\n",
    "    print(\"--- Starting Evaluation ---\")\n",
    "    all_evaluations = []\n",
    "    # State에서 benchmark_data 또는 그 매핑을 가져와야 함\n",
    "    # 예: benchmark_data가 evaluate_responses 스코프에서 사용 가능하다고 가정\n",
    "    benchmark_map_full = {item[\"id\"]: item for item in benchmark_data}\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "    evaluation_chain = evaluation_prompt_template | evaluator | parser\n",
    "\n",
    "    model_result_keys = [key for key in state if key.endswith(\"_results\") and key != \"evaluation_results\"]\n",
    "\n",
    "    for key in model_result_keys:\n",
    "        print(f\"  Evaluating results from: {key}\")\n",
    "        model_results = state.get(key, [])\n",
    "        for response_item in model_results:\n",
    "            prompt_id = response_item.get(\"id\")\n",
    "            model_name = response_item.get(\"model_name\")\n",
    "            response_content = response_item.get(\"response\")\n",
    "            error = response_item.get(\"error\")\n",
    "\n",
    "            if error:\n",
    "                eval_result = {\"id\": prompt_id, \"agent_name\": model_name, \"score\": None, \"rationale\": f\"Skipped due to error: {error}\", \"error\": True}\n",
    "                all_evaluations.append(eval_result)\n",
    "                continue\n",
    "\n",
    "            if not prompt_id or prompt_id not in benchmark_map_full:\n",
    "                print(f\"    Warning: Missing full benchmark data for prompt ID {prompt_id}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            benchmark_item = benchmark_map_full[prompt_id] # 해당 ID의 전체 benchmark 데이터\n",
    "\n",
    "            if not response_content:\n",
    "                 eval_result = {\"id\": prompt_id, \"agent_name\": model_name, \"score\": 0, \"rationale\": \"Empty response\", \"error\": False}\n",
    "                 all_evaluations.append(eval_result)\n",
    "                 continue\n",
    "\n",
    "            # --- 여기가 중요: input_data 딕셔너리 생성 ---\n",
    "            # benchmark_item (JSON 파일의 항목) 과 response_content (모델 응답)에서 값을 가져와\n",
    "            # evaluation_prompt_template 의 변수 이름에 매핑합니다.\n",
    "            input_data = {\n",
    "                \"task_description\": benchmark_item.get(\"task\", \"N/A\"),              # JSON의 'task' 필드\n",
    "                \"user_query\": benchmark_item.get(\"input\", \"N/A\"),                # JSON의 'input' 필드\n",
    "                \"reference_answer\": benchmark_item.get(\"reference_answer\", \"N/A\"),# JSON의 'reference_answer' 필드\n",
    "                \"ai_response\": response_content,                                 # LangGraph State에서 온 모델 응답\n",
    "                \"criteria\": benchmark_item.get(\"score_rubric\", {}).get(\"criteria\", \"N/A\"), # JSON의 'score_rubric'.'criteria'\n",
    "                \"score1_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score1_description\", \"N/A\"), # 이하 scoreX_description\n",
    "                \"score2_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score2_description\", \"N/A\"),\n",
    "                \"score3_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score3_description\", \"N/A\"),\n",
    "                \"score4_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score4_description\", \"N/A\"),\n",
    "                \"score5_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score5_description\", \"N/A\"),\n",
    "            }\n",
    "            # ---------------------------------------------\n",
    "\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                evaluation_output_str = evaluation_chain.invoke(input_data)\n",
    "                end_time = time.time()\n",
    "\n",
    "                # 출력 파싱 (이전 답변과 동일)\n",
    "                score = None\n",
    "                rationale = \"\"\n",
    "                score_match = re.search(r\"Score:\\s*(\\d)\", evaluation_output_str)\n",
    "                rationale_match = re.search(r\"Rationale:\\s*(.*)\", evaluation_output_str, re.DOTALL)\n",
    "\n",
    "                if score_match:\n",
    "                    score = int(score_match.group(1))\n",
    "                if rationale_match:\n",
    "                    rationale = rationale_match.group(1).strip()\n",
    "\n",
    "                if score is None or not rationale:\n",
    "                     print(f\"    Warning: Could not parse score/rationale for prompt {prompt_id}. Raw output: {evaluation_output_str}\")\n",
    "                     rationale = f\"Parsing Warning. Raw Output: {evaluation_output_str}\"\n",
    "\n",
    "                eval_result = {\n",
    "                    \"id\": prompt_id, \"agent_name\": model_name, \"score\": score,\n",
    "                    \"rationale\": rationale, \"latency\": end_time - start_time, \"error\": False\n",
    "                }\n",
    "                print(f\"    Evaluated prompt {prompt_id} from {model_name} in {eval_result['latency']:.2f}s. Score: {eval_result['score']}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Error evaluating prompt {prompt_id} from {model_name}: {e}\")\n",
    "                eval_result = {\n",
    "                    \"id\": prompt_id, \"agent_name\": model_name, \"score\": None,\n",
    "                    \"rationale\": f\"Evaluation failed: {str(e)}\", \"latency\": 0, \"error\": True\n",
    "                }\n",
    "            all_evaluations.append(eval_result)\n",
    "\n",
    "    print(\"--- Evaluation Finished ---\")\n",
    "    return {\"evaluation_results\": all_evaluations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- define workflow ---\n",
    "\n",
    "# 1. 워크플로우 시작 전에 타임스탬프 생성\n",
    "current_timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# 2. initial state 설정 시 생성된 타임스탬프 포함\n",
    "initial_state = {\n",
    "    \"prompts\": prompts,\n",
    "    \"rubrics\": rubrics,\n",
    "#    \"processed_count\": 0,\n",
    "    \"phi_results\": [],\n",
    "    \"qwen1_5_results\": [],\n",
    "    \"vicuna_results\": [],\n",
    "    \"llama2_results\": [],\n",
    "    \"llama3_1_results\": [],\n",
    "    \"gemma3_results\": [],\n",
    "    # \"deepseek_r1_results\": [],\n",
    "    \"evaluation_results\": [],\n",
    "    \"timestamp\": current_timestamp  # 생성된 타임스탬프를 State에 추가\n",
    "}\n",
    "\n",
    "# Map rubrics by ID for easier access in evaluation if needed directly from state\n",
    "# Although the current evaluate_responses uses benchmark_map loaded from the file\n",
    "initial_state[\"rubrics\"] = {item[\"id\"]: item for item in rubrics}\n",
    "\n",
    "# create workflow (기존과 동일)\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# add nodes\n",
    "workflow.add_node(\"process_phi\", process_phi)\n",
    "workflow.add_node(\"process_qwen1_5\", process_qwen1_5)\n",
    "workflow.add_node(\"process_vicuna\", process_vicuna)\n",
    "workflow.add_node(\"process_llama2\", process_llama2)\n",
    "workflow.add_node(\"process_llama3\", process_llama3_1)\n",
    "workflow.add_node(\"process_gemma3\", process_gemma3)\n",
    "workflow.add_node(\"save_responses\", save_intermediate_results)\n",
    "workflow.add_node(\"evaluate\", evaluate_responses)\n",
    "\n",
    "# connect edges\n",
    "workflow.set_entry_point(\"process_phi\")\n",
    "workflow.add_edge(\"process_phi\", \"process_qwen1_5\")\n",
    "workflow.add_edge(\"process_qwen1_5\", \"process_vicuna\")\n",
    "workflow.add_edge(\"process_vicuna\", \"process_llama2\")\n",
    "workflow.add_edge(\"process_llama2\", \"process_llama3\")\n",
    "workflow.add_edge(\"process_llama3\", \"process_gemma3\")\n",
    "workflow.add_edge(\"process_gemma3\", \"save_responses\")\n",
    "workflow.add_edge(\"save_responses\", \"evaluate\")\n",
    "workflow.add_edge(\"evaluate\", END)\n",
    "\n",
    "\n",
    "# compile workflow (기존과 동일)\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run workflow and save the final state\n",
    "print(\"--- Starting Workflow ---\")\n",
    "final_state = app.invoke(initial_state)\n",
    "print(\"--- Workflow Finished ---\")\n",
    "\n",
    "# --- save final results (using timestamp from final_state) ---\n",
    "final_output_dir = \"_output\"\n",
    "os.makedirs(final_output_dir, exist_ok=True)\n",
    "\n",
    "# 3. 최종 상태에서 타임스탬프 가져와서 사용\n",
    "final_timestamp = final_state.get(\"timestamp\", datetime.now().strftime(\"%Y%m%d%H%M%S_final_fallback\"))\n",
    "if not final_timestamp:\n",
    "    print(\"  Warning: Timestamp not found in final state. Generating a new one for fallback.\")\n",
    "    final_timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S_final_fallback\")\n",
    "\n",
    "\n",
    "final_base_name, _ = os.path.splitext(os.path.basename(bench_path)) # 파일명 생성시 bench_path에서 확장자 제거\n",
    "final_filename = f\"{final_timestamp}_{final_base_name}_with_evaluation.json\" # 수정됨\n",
    "final_output_file_path = os.path.join(final_output_dir, final_filename)\n",
    "\n",
    "try:\n",
    "    with open(final_output_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_state, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\nFinal results (with evaluation) successfully saved to {final_output_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving final results: {e}\")\n",
    "    print(f\"Final state type: {type(final_state)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
