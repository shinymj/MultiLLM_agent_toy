{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "\n",
    "o3_mini = init_chat_model(\"openai:o3-mini\")\n",
    "claude_sonnet = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0)\n",
    "gemma3 = init_chat_model(\"ollama:gemma3:12b\", temperature=0)\n",
    "llama3_1 = init_chat_model(\"ollama:llama3.1:latest\", temperature=0)\n",
    "deepseek_r1 = init_chat_model(\"ollama:deepseek-r1:8b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing benchmark data\n",
    "\n",
    "file_path = \"biggen_bench_instruction_idx0.json\"\n",
    "\n",
    "def load_benchmark_data(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Load benchmark data from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def prepare_prompts(benchmark_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Prepare prompts by excluding reference_answer and score_rubric.\"\"\"\n",
    "    prompts = []\n",
    "    for item in benchmark_data:\n",
    "        prompt = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"capability\": item[\"capability\"],\n",
    "            \"task\": item[\"task\"],\n",
    "            \"instance_idx\": item[\"instance_idx\"],\n",
    "            \"system_prompt\": item[\"system_prompt\"],\n",
    "            \"input\": item[\"input\"],\n",
    "            # Exclude reference_answer and score_rubric\n",
    "        }\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "def prepare_rubric(benchmark_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Prepare rubric including reference_answer and score_rubric.\"\"\"\n",
    "    rubric = []\n",
    "    for item in benchmark_data:\n",
    "        prompt = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"reference_answer\": item[\"reference_answer\"],\n",
    "            \"score_rubric\": item[\"score_rubric\"]\n",
    "        }\n",
    "        rubric.append(prompt)\n",
    "    return rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function\n",
    "benchmark_data = load_benchmark_data(file_path)\n",
    "prompts = prepare_prompts(benchmark_data)\n",
    "rubric = prepare_rubric(benchmark_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state structure\n",
    "class State(TypedDict):\n",
    "    prompts: List[Dict[str, Any]]\n",
    "    processed_count: int\n",
    "    gemma3_results: List[Dict[str, Any]]\n",
    "    llama3_1_results: List[Dict[str, Any]]\n",
    "    deepseek_r1_results: List[Dict[str, Any]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_processor(model, model_name):\n",
    "    def process_model(state: State) -> State:\n",
    "        results = []\n",
    "        \n",
    "        for prompt in state[\"prompts\"]:\n",
    "            system_prompt = prompt.get('system_prompt', '')\n",
    "            user_input = prompt.get('input', '')\n",
    "            \n",
    "            try:\n",
    "                response = model.invoke(\n",
    "                    user_input,\n",
    "                    config={\"system_prompt\": system_prompt}\n",
    "                )\n",
    "                \n",
    "                response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "                result = {\n",
    "                    \"id\": prompt.get('id', ''),\n",
    "                    \"model_name\": model_name,\n",
    "                    \"response\": response_content\n",
    "                }\n",
    "            except Exception as e:\n",
    "                result = {\n",
    "                    \"id\": prompt.get('id', ''),\n",
    "                    \"model_name\": model_name,\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "                \n",
    "            results.append(result)\n",
    "        \n",
    "        results_key = f\"{model_name}_results\"\n",
    "        return {\n",
    "            **state,\n",
    "            results_key: results\n",
    "        }\n",
    "    \n",
    "    return process_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_gemma3 = create_model_processor(gemma3, \"gemma3\")\n",
    "process_llama3_1 = create_model_processor(llama3_1, \"llama3_1\")\n",
    "process_deepseek_r1 = create_model_processor(deepseek_r1, \"deepseek_r1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 상태 설정\n",
    "initial_state = {\n",
    "    \"prompts\": prompts,\n",
    "    \"processed_count\": 0,\n",
    "    \"gemma3_results\": [],\n",
    "    \"llama3_1_results\": [],\n",
    "    \"deepseek_r1_results\": []\n",
    "}\n",
    "\n",
    "# 그래프 생성\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"process_gemma3\", process_gemma3)\n",
    "workflow.add_node(\"process_llama3\", process_llama3_1)\n",
    "workflow.add_node(\"process_deepseek\", process_deepseek_r1)\n",
    "\n",
    "# 노드 연결\n",
    "workflow.set_entry_point(\"process_gemma3\")\n",
    "workflow.add_edge(\"process_gemma3\", \"process_llama3\")\n",
    "workflow.add_edge(\"process_llama3\", \"process_deepseek\")\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = workflow.compile()\n",
    "\n",
    "# 실행\n",
    "final_state = app.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_state가 dict 타입인지 확인\n",
    "if isinstance(final_state, dict):\n",
    "    # JSON 파일로 저장\n",
    "    output_file_path = \"output_parallel_results.json\"\n",
    "    try:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_state, f, ensure_ascii=False, indent=4) # ensure_ascii=False는 한글 깨짐 방지, indent는 가독성 향상\n",
    "        print(f\"결과가 {output_file_path} 에 성공적으로 저장되었습니다.\")\n",
    "    except TypeError as e:\n",
    "        print(f\"JSON 직렬화 오류: {e}\")\n",
    "        # 만약 final_state 딕셔너리 내부에 JSON으로 변환할 수 없는 타입 (예: 모델 객체 자체)이 있다면\n",
    "        # 해당 부분을 처리해야 합니다. 현재 코드에서는 LLM 응답(response)이 문자열 또는 LangChain 메시지 객체일 가능성이 높으며,\n",
    "        # LangChain 메시지 객체는 기본적으로 JSON 직렬화가 어려울 수 있습니다.\n",
    "        # 이 경우, response 내용을 문자열 등으로 변환하는 과정이 필요할 수 있습니다.\n",
    "        # 예를 들어, process_model 함수 내에서 response를 저장할 때:\n",
    "        # \"response\": response.content # AIMessage 등의 객체일 경우 .content 사용\n",
    "else:\n",
    "    print(f\"오류: 최종 결과의 타입이 dict가 아닙니다. 타입: {type(final_state)}\")\n",
    "    # 만약 정말로 str이라면, 어떤 과정에서 문자열로 변환되었는지 확인 필요\n",
    "    # 예: final_state = str(app.invoke(initial_state)) 와 같이 실수로 변환했을 수 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
