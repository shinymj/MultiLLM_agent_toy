{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chat models\n",
    "# o3_mini = init_chat_model(\"openai:o3-mini\")\n",
    "# claude_haiku = init_chat_model(\"anthropic:claude-3-haiku-20240307\", temperature=0)\n",
    "#gemma3 = init_chat_model(\"ollama:gemma3:12b\", temperature=0)\n",
    "llama2 = init_chat_model(\"ollama:llama2:latest\", temperature=0)\n",
    "llama3_1 = init_chat_model(\"ollama:llama3.1:latest\", temperature=0)\n",
    "#deepseek_r1 = init_chat_model(\"ollama:deepseek-r1:8b\", temperature=0)\n",
    "answerer = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0)\n",
    "evaluator = init_chat_model(\"anthropic:claude-3-7-sonnet-20250219\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing benchmark data and prompts, rubric\n",
    "\n",
    "bench_path = \"biggen_bench_instruction_idx0.json\"\n",
    "\n",
    "def load_benchmark_data(bench_path: str) -> List[Dict]:\n",
    "    \"\"\"Load benchmark data from a JSON file.\"\"\"\n",
    "    with open(bench_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "benchmark_data = load_benchmark_data(bench_path)\n",
    "\n",
    "def prepare_prompts(benchmark_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Prepare prompts by excluding reference_answer and score_rubric.\"\"\"\n",
    "    prompts = []\n",
    "    for item in benchmark_data:\n",
    "        prompt = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"capability\": item[\"capability\"],\n",
    "            \"task\": item[\"task\"],\n",
    "            \"instance_idx\": item[\"instance_idx\"],\n",
    "            \"system_prompt\": item[\"system_prompt\"],\n",
    "            \"input\": item[\"input\"],\n",
    "            # Exclude reference_answer and score_rubric\n",
    "        }\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "def prepare_rubric(benchmark_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Prepare rubric including reference_answer and score_rubric.\"\"\"\n",
    "    rubric = []\n",
    "    for item in benchmark_data:\n",
    "        prompt = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"reference_answer\": item[\"reference_answer\"],\n",
    "            \"score_rubric\": item[\"score_rubric\"]\n",
    "        }\n",
    "        rubric.append(prompt)\n",
    "    return rubric\n",
    "\n",
    "prompts = prepare_prompts(benchmark_data)\n",
    "rubrics = prepare_rubric(benchmark_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state structure\n",
    "class State(TypedDict):\n",
    "    prompts: List[Dict[str, Any]]\n",
    "    rubrics: Dict[str, Dict[str, Any]] \n",
    "    processed_count: int\n",
    "#    gemma3_results: List[Dict[str, Any]]\n",
    "    llama2_results: List[Dict[str, Any]]\n",
    "    llama3_1_results: List[Dict[str, Any]]\n",
    "#    deepseek_r1_results: List[Dict[str, Any]]\n",
    "    evaluation_results: List[Dict[str, Any]]\n",
    "    timestamp: str  # 워크플로우 전체에서 공유할 타임스탬프 필드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuestionAugmentedProcessor Class to replace the create_model_processor function\n",
    "class QuestionAugmentedProcessor:\n",
    "    def __init__(self, model, model_name, answerer, benchmark_data):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.answerer = answerer\n",
    "        self.benchmark_data = benchmark_data\n",
    "        self.benchmark_map = {item[\"id\"]: item for item in benchmark_data}\n",
    "    \n",
    "    def generate_uncertainty_check(self, system_prompt, user_input):\n",
    "        \"\"\"Check if the model needs additional clarification before answering.\"\"\"\n",
    "        uncertainty_prompt = f\"\"\"\n",
    "        Given the following task:\n",
    "        \n",
    "        \"{user_input}\"\n",
    "        \n",
    "        Your goal is to determine if you need any additional information or clarification to provide a comprehensive response.\n",
    "        \n",
    "        If you have all the information needed to respond confidently, reply with \"None\".\n",
    "        If you need clarification, provide a brief phrase describing what specific information you need.\n",
    "        Keep your response brief and focused.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            uncertainty_response = self.model.invoke(\n",
    "                uncertainty_prompt,\n",
    "                config={\"system_prompt\": system_prompt}\n",
    "            )\n",
    "            \n",
    "            # Extract content from response\n",
    "            if isinstance(uncertainty_response, BaseMessage) and hasattr(uncertainty_response, 'content'):\n",
    "                uncertainty = uncertainty_response.content\n",
    "            elif isinstance(uncertainty_response, str):\n",
    "                uncertainty = uncertainty_response\n",
    "            elif isinstance(uncertainty_response, dict) and 'content' in uncertainty_response:\n",
    "                uncertainty = uncertainty_response['content']\n",
    "            else:\n",
    "                uncertainty = str(uncertainty_response)\n",
    "            \n",
    "            # Clean up and standardize the response\n",
    "            uncertainty = uncertainty.strip()\n",
    "            if uncertainty.lower() in [\"none\", \"no\", \"no uncertainty\", \"i don't need any additional information\"]:\n",
    "                return None\n",
    "            return uncertainty\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error checking uncertainty: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_question(self, system_prompt, user_input, uncertainty, question_num):\n",
    "        \"\"\"Generate a clarifying question based on uncertainty.\"\"\"\n",
    "        question_prompt = f\"\"\"\n",
    "        Given the following task:\n",
    "        \n",
    "        \"{user_input}\"\n",
    "        \n",
    "        You have identified the following uncertainty: \"{uncertainty}\"\n",
    "        \n",
    "        Formulate a clear, specific question (Question #{question_num+1}) that would help resolve this uncertainty.\n",
    "        Your question should be brief, focused, and directly related to the uncertainty.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            question_response = self.model.invoke(\n",
    "                question_prompt,\n",
    "                config={\"system_prompt\": system_prompt}\n",
    "            )\n",
    "            \n",
    "            # Extract content from response\n",
    "            if isinstance(question_response, BaseMessage) and hasattr(question_response, 'content'):\n",
    "                question = question_response.content\n",
    "            elif isinstance(question_response, str):\n",
    "                question = question_response\n",
    "            elif isinstance(question_response, dict) and 'content' in question_response:\n",
    "                question = question_response['content']\n",
    "            else:\n",
    "                question = str(question_response)\n",
    "                \n",
    "            return question.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating question: {e}\")\n",
    "            return f\"Error generating question {question_num+1}.\"\n",
    "    \n",
    "    def generate_answer(self, prompt_id, question, user_input):\n",
    "        \"\"\"Generate an answer to a clarifying question using the answerer model.\"\"\"\n",
    "        # Get reference data\n",
    "        benchmark_item = self.benchmark_map.get(prompt_id, {})\n",
    "        reference_answer = benchmark_item.get(\"reference_answer\", \"\")\n",
    "        \n",
    "        answer_prompt = f\"\"\"\n",
    "        You are providing clarification to an AI that is working on the following task:\n",
    "        \n",
    "        \"{user_input}\"\n",
    "        \n",
    "        The AI has asked: \"{question}\"\n",
    "        \n",
    "        Provide a brief, focused answer to this question that offers useful clarification without solving the entire task.\n",
    "        Use the following reference information to inform your answer, but don't simply state the final answer:\n",
    "        \n",
    "        REFERENCE (DO NOT EXPLICITLY SHARE THIS): {reference_answer}\n",
    "        \n",
    "        Your response should be helpful but not provide the complete solution directly.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            answer_response = self.answerer.invoke(answer_prompt)\n",
    "            \n",
    "            # Extract content from response\n",
    "            if isinstance(answer_response, BaseMessage) and hasattr(answer_response, 'content'):\n",
    "                answer = answer_response.content\n",
    "            elif isinstance(answer_response, str):\n",
    "                answer = answer_response\n",
    "            elif isinstance(answer_response, dict) and 'content' in answer_response:\n",
    "                answer = answer_response['content']\n",
    "            else:\n",
    "                answer = str(answer_response)\n",
    "                \n",
    "            return answer.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating answer: {e}\")\n",
    "            return f\"Error generating answer to question.\"\n",
    "    \n",
    "    def generate_final_response(self, system_prompt, user_input, inquiring_list):\n",
    "        \"\"\"Generate the final response based on original input and inquiring results.\"\"\"\n",
    "        # Build context from inquiring list\n",
    "        inquiring_context = \"\"\n",
    "        for i, inquiry in enumerate(inquiring_list):\n",
    "            if \"question_\" + str(i) in inquiry and \"answer_\" + str(i) in inquiry:\n",
    "                inquiring_context += f\"\\nQuestion {i+1}: {inquiry['question_' + str(i)]}\\n\"\n",
    "                inquiring_context += f\"Answer {i+1}: {inquiry['answer_' + str(i)]}\\n\"\n",
    "        \n",
    "        final_prompt = f\"\"\"\n",
    "        Given the following task:\n",
    "        \n",
    "        \"{user_input}\"\n",
    "        \n",
    "        And the following additional context from clarifying questions and answers:\n",
    "        {inquiring_context if inquiring_context else \"No additional context available.\"}\n",
    "        \n",
    "        Provide your complete and final response to the original task.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model.invoke(\n",
    "                final_prompt,\n",
    "                config={\"system_prompt\": system_prompt}\n",
    "            )\n",
    "            \n",
    "            # Extract content from response\n",
    "            if isinstance(response, BaseMessage) and hasattr(response, 'content'):\n",
    "                final_response = response.content\n",
    "            elif isinstance(response, str):\n",
    "                final_response = response\n",
    "            elif isinstance(response, dict) and 'content' in response:\n",
    "                final_response = response['content']\n",
    "            else:\n",
    "                final_response = str(response)\n",
    "                \n",
    "            return final_response.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating final response: {e}\")\n",
    "            return \"Error generating final response.\"\n",
    "    \n",
    "    def process(self, state):\n",
    "        \"\"\"Main processor method that handles the entire QAG workflow.\"\"\"\n",
    "        results = []\n",
    "        print(f\"--- Processing model: {self.model_name} with Question Augmented Generation ---\")\n",
    "        \n",
    "        prompts_to_process = state.get(\"prompts\", [])\n",
    "        if not prompts_to_process:\n",
    "            print(f\"  No prompts found for {self.model_name}.\")\n",
    "            return {}  # Return empty if no prompts\n",
    "        \n",
    "        for prompt in prompts_to_process:\n",
    "            prompt_id = prompt.get('id', 'unknown_id')\n",
    "            system_prompt = prompt.get('system_prompt', '')\n",
    "            user_input = prompt.get('input', '')\n",
    "            \n",
    "            if not user_input:\n",
    "                print(f\"  Skipping prompt {prompt_id} for {self.model_name} due to empty input.\")\n",
    "                result = {\n",
    "                    \"id\": prompt_id,\n",
    "                    \"model_name\": self.model_name,\n",
    "                    \"inquiring\": [],\n",
    "                    \"response\": None,\n",
    "                    \"error\": \"Skipped due to empty input\",\n",
    "                    \"latency\": 0\n",
    "                }\n",
    "                results.append(result)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Initialize inquiring list to track the QAG process\n",
    "                inquiring_list = []\n",
    "                \n",
    "                # Check for uncertainties that need clarification\n",
    "                uncertainty = self.generate_uncertainty_check(system_prompt, user_input)\n",
    "                \n",
    "                # If there's uncertainty, start the QAG process\n",
    "                question_count = 0\n",
    "                max_questions = 2  # Maximum 3 questions (0, 1, 2)\n",
    "                \n",
    "                while uncertainty and question_count <= max_questions:\n",
    "                    # Create a new inquiry entry\n",
    "                    inquiry = {\"uncertainty\": uncertainty}\n",
    "                    \n",
    "                    # Generate question\n",
    "                    question = self.generate_question(system_prompt, user_input, uncertainty, question_count)\n",
    "                    inquiry[f\"question_{question_count}\"] = question\n",
    "                    \n",
    "                    # Generate answer from the answerer model\n",
    "                    answer = self.generate_answer(prompt_id, question, user_input)\n",
    "                    inquiry[f\"answer_{question_count}\"] = answer\n",
    "                    \n",
    "                    # Add this inquiry to the list\n",
    "                    inquiring_list.append(inquiry)\n",
    "                    \n",
    "                    # Check if we need further clarification\n",
    "                    if question_count < max_questions:\n",
    "                        uncertainty = self.generate_uncertainty_check(system_prompt, user_input)\n",
    "                    else:\n",
    "                        uncertainty = None  # Stop after max questions\n",
    "                    \n",
    "                    question_count += 1\n",
    "                \n",
    "                # Generate the final response\n",
    "                final_response = self.generate_final_response(system_prompt, user_input, inquiring_list)\n",
    "                \n",
    "                end_time = time.time()\n",
    "                latency = end_time - start_time\n",
    "                \n",
    "                # Create result object with the inquiring list and final response\n",
    "                result = {\n",
    "                    \"id\": prompt_id,\n",
    "                    \"model_name\": self.model_name,\n",
    "                    \"inquiring\": inquiring_list,\n",
    "                    \"response\": final_response,\n",
    "                    \"latency\": latency,\n",
    "                    \"error\": None\n",
    "                }\n",
    "                \n",
    "                print(f\"  Successfully processed prompt {prompt_id} for {self.model_name} in {result['latency']:.2f}s with {len(inquiring_list)} inquiries\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_message = f\"Error processing prompt {prompt_id} for {self.model_name}: {type(e).__name__}: {e}\"\n",
    "                print(f\"  {error_message}\")\n",
    "                \n",
    "                result = {\n",
    "                    \"id\": prompt_id,\n",
    "                    \"model_name\": self.model_name,\n",
    "                    \"inquiring\": [],  # Empty inquiring list on error\n",
    "                    \"response\": None,\n",
    "                    \"error\": error_message,\n",
    "                    \"latency\": 0\n",
    "                }\n",
    "                \n",
    "            results.append(result)\n",
    "        \n",
    "        results_key = f\"{self.model_name}_results\"\n",
    "        return {results_key: results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create processors using the new class\n",
    "llama2_processor = QuestionAugmentedProcessor(llama2, \"llama2\", answerer, benchmark_data)\n",
    "llama3_1_processor = QuestionAugmentedProcessor(llama3_1, \"llama3_1\", answerer, benchmark_data)\n",
    "# process_deepseek_r1 = create_model_processor(deepseek_r1, \"deepseek_r1\")\n",
    "\n",
    "# Define the process functions that will be used as nodes\n",
    "def process_llama2(state):\n",
    "    return llama2_processor.process(state)\n",
    "\n",
    "def process_llama3_1(state):\n",
    "    return llama3_1_processor.process(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- define save intermediate results node ---\n",
    "def save_intermediate_results(state: State) -> Dict:\n",
    "    \"\"\"Saves the generated responses before evaluation using the timestamp from the state.\"\"\"\n",
    "    print(\"--- Saving Intermediate Results (Responses Only) ---\")\n",
    "    # State로부터 타임스탬프 가져오기\n",
    "    timestamp = state.get(\"timestamp\", datetime.now().strftime(\"%Y%m%d%H%M%S_fallback\"))\n",
    "    if not timestamp:\n",
    "        print(\"  Warning: Timestamp not found in state. Generating a new one for fallback.\")\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S_fallback\")\n",
    "    \n",
    "    output_dir = \"_output\"\n",
    "    os.makedirs(output_dir, exist_ok=True) # 출력 디렉토리 확인 및 생성\n",
    "\n",
    "    # 저장할 데이터 구성 (응답 결과만 선택)\n",
    "    data_to_save = {}\n",
    "    model_result_keys = [key for key in state if key.endswith(\"_results\") and key not in [\"evaluation_results\"]]\n",
    "    for key in model_result_keys:\n",
    "        if state.get(key):\n",
    "             data_to_save[key] = state[key]\n",
    "\n",
    "    # 프롬프트 정보도 함께 저장하고 싶다면 추가\n",
    "    # data_to_save[\"prompts\"] = state.get(\"prompts\", [])\n",
    "\n",
    "    if not data_to_save:\n",
    "        print(\"  No response data found to save.\")\n",
    "        return {} # 저장할 데이터 없으면 아무것도 안함\n",
    "\n",
    "    base_name, _ = os.path.splitext(os.path.basename(bench_path)) # 파일명 생성시 bench_path에서 확장자 제거\n",
    "    intermediate_filename = f\"{timestamp}_{base_name}_responses_only.json\" # 수정됨\n",
    "    output_file_path = os.path.join(output_dir, intermediate_filename)\n",
    "\n",
    "    try:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data_to_save, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"  Intermediate results successfully saved to {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error saving intermediate results: {e}\")\n",
    "\n",
    "    # 이 노드는 상태를 변경하지 않으므로 빈 딕셔셔리 반환\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Evaluation node ---\n",
    "\n",
    "# 평가 프롬프트 템플릿 정의 (biggen_bench_instruction_idx0.json 구조 기반)\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert evaluator. Your task is to evaluate an AI assistant's response based on the provided user query, reference answer, and a detailed scoring rubric.\n",
    "Focus ONLY on the provided information and rubric. Assign a score from 1 to 5, where 5 is the best, according to the descriptions.\n",
    "Provide your output strictly in the specified format.\"\"\"),\n",
    "    (\"human\", \"\"\"\n",
    "**Evaluation Context:**\n",
    "\n",
    "* **Task Type:** {task_description}\n",
    "* **User Query:**\n",
    "    ```\n",
    "    {user_query}\n",
    "    ```\n",
    "* **Reference Answer:**\n",
    "    ```\n",
    "    {reference_answer}\n",
    "    ```\n",
    "* **AI Response to Evaluate:**\n",
    "    ```\n",
    "    {ai_response}\n",
    "    ```\n",
    "\n",
    "**Scoring Rubric:**\n",
    "\n",
    "* **Criteria:** {criteria}\n",
    "* **Score 1 Description:** {score1_desc}\n",
    "* **Score 2 Description:** {score2_desc}\n",
    "* **Score 3 Description:** {score3_desc}\n",
    "* **Score 4 Description:** {score4_desc}\n",
    "* **Score 5 Description:** {score5_desc}\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  Carefully compare the \"AI Response to Evaluate\" against the \"Reference Answer\" and the \"Scoring Rubric\".\n",
    "2.  Determine the score (1-5) that best reflects the quality of the AI Response according to the rubric descriptions.\n",
    "3.  Provide a brief rationale explaining *why* you chose that score, referencing specific aspects of the rubric descriptions and the AI response.\n",
    "\n",
    "**Output Format (MUST follow exactly):**\n",
    "Score: [Your score between 1-5]\n",
    "Rationale: [Your concise explanation based on the rubric]\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "# --- evaluate_responses 함수 내에서 이 템플릿을 사용하는 방법 ---\n",
    "\n",
    "def evaluate_responses(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluates responses from different models based on rubrics.\"\"\"\n",
    "    print(\"--- Starting Evaluation ---\")\n",
    "    all_evaluations = []\n",
    "    # State에서 benchmark_data 또는 그 매핑을 가져와야 함\n",
    "    # 예: benchmark_data가 evaluate_responses 스코프에서 사용 가능하다고 가정\n",
    "    benchmark_map_full = {item[\"id\"]: item for item in benchmark_data}\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "    evaluation_chain = evaluation_prompt_template | evaluator | parser\n",
    "\n",
    "    model_result_keys = [key for key in state if key.endswith(\"_results\") and key != \"evaluation_results\"]\n",
    "\n",
    "    for key in model_result_keys:\n",
    "        print(f\"  Evaluating results from: {key}\")\n",
    "        model_results = state.get(key, [])\n",
    "        for response_item in model_results:\n",
    "            prompt_id = response_item.get(\"id\")\n",
    "            model_name = response_item.get(\"model_name\")\n",
    "            response_content = response_item.get(\"response\")\n",
    "            error = response_item.get(\"error\")\n",
    "\n",
    "            if error:\n",
    "                eval_result = {\"id\": prompt_id, \"agent_name\": model_name, \"score\": None, \"rationale\": f\"Skipped due to error: {error}\", \"error\": True}\n",
    "                all_evaluations.append(eval_result)\n",
    "                continue\n",
    "\n",
    "            if not prompt_id or prompt_id not in benchmark_map_full:\n",
    "                print(f\"    Warning: Missing full benchmark data for prompt ID {prompt_id}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            benchmark_item = benchmark_map_full[prompt_id] # 해당 ID의 전체 benchmark 데이터\n",
    "\n",
    "            if not response_content:\n",
    "                 eval_result = {\"id\": prompt_id, \"agent_name\": model_name, \"score\": 0, \"rationale\": \"Empty response\", \"error\": False}\n",
    "                 all_evaluations.append(eval_result)\n",
    "                 continue\n",
    "\n",
    "            # --- 여기가 중요: input_data 딕셔너리 생성 ---\n",
    "            # benchmark_item (JSON 파일의 항목) 과 response_content (모델 응답)에서 값을 가져와\n",
    "            # evaluation_prompt_template 의 변수 이름에 매핑합니다.\n",
    "            input_data = {\n",
    "                \"task_description\": benchmark_item.get(\"task\", \"N/A\"),              # JSON의 'task' 필드\n",
    "                \"user_query\": benchmark_item.get(\"input\", \"N/A\"),                # JSON의 'input' 필드\n",
    "                \"reference_answer\": benchmark_item.get(\"reference_answer\", \"N/A\"),# JSON의 'reference_answer' 필드\n",
    "                \"ai_response\": response_content,                                 # LangGraph State에서 온 모델 응답\n",
    "                \"criteria\": benchmark_item.get(\"score_rubric\", {}).get(\"criteria\", \"N/A\"), # JSON의 'score_rubric'.'criteria'\n",
    "                \"score1_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score1_description\", \"N/A\"), # 이하 scoreX_description\n",
    "                \"score2_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score2_description\", \"N/A\"),\n",
    "                \"score3_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score3_description\", \"N/A\"),\n",
    "                \"score4_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score4_description\", \"N/A\"),\n",
    "                \"score5_desc\": benchmark_item.get(\"score_rubric\", {}).get(\"score5_description\", \"N/A\"),\n",
    "            }\n",
    "            # ---------------------------------------------\n",
    "\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                evaluation_output_str = evaluation_chain.invoke(input_data)\n",
    "                end_time = time.time()\n",
    "\n",
    "                # 출력 파싱 (이전 답변과 동일)\n",
    "                score = None\n",
    "                rationale = \"\"\n",
    "                score_match = re.search(r\"Score:\\s*(\\d)\", evaluation_output_str)\n",
    "                rationale_match = re.search(r\"Rationale:\\s*(.*)\", evaluation_output_str, re.DOTALL)\n",
    "\n",
    "                if score_match:\n",
    "                    score = int(score_match.group(1))\n",
    "                if rationale_match:\n",
    "                    rationale = rationale_match.group(1).strip()\n",
    "\n",
    "                if score is None or not rationale:\n",
    "                     print(f\"    Warning: Could not parse score/rationale for prompt {prompt_id}. Raw output: {evaluation_output_str}\")\n",
    "                     rationale = f\"Parsing Warning. Raw Output: {evaluation_output_str}\"\n",
    "\n",
    "                eval_result = {\n",
    "                    \"id\": prompt_id, \"agent_name\": model_name, \"score\": score,\n",
    "                    \"rationale\": rationale, \"latency\": end_time - start_time, \"error\": False\n",
    "                }\n",
    "                print(f\"    Evaluated prompt {prompt_id} from {model_name} in {eval_result['latency']:.2f}s. Score: {eval_result['score']}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Error evaluating prompt {prompt_id} from {model_name}: {e}\")\n",
    "                eval_result = {\n",
    "                    \"id\": prompt_id, \"agent_name\": model_name, \"score\": None,\n",
    "                    \"rationale\": f\"Evaluation failed: {str(e)}\", \"latency\": 0, \"error\": True\n",
    "                }\n",
    "            all_evaluations.append(eval_result)\n",
    "\n",
    "    print(\"--- Evaluation Finished ---\")\n",
    "    return {\"evaluation_results\": all_evaluations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- define workflow and run ---\n",
    "\n",
    "# 1. 워크플로우 시작 전에 타임스탬프 생성\n",
    "current_timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# 2. initial state 설정 시 생성된 타임스탬프 포함\n",
    "initial_state = {\n",
    "    \"prompts\": prompts,\n",
    "    \"rubrics\": rubrics,\n",
    "#    \"processed_count\": 0,\n",
    "    \"llama2_results\": [],\n",
    "    \"llama3_1_results\": [],\n",
    "    # \"deepseek_r1_results\": [],\n",
    "    \"evaluation_results\": [],\n",
    "    \"timestamp\": current_timestamp  # 생성된 타임스탬프를 State에 추가\n",
    "}\n",
    "\n",
    "# create workflow (기존과 동일)\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# 노드 추가 (기존과 동일)\n",
    "# workflow.add_node(\"process_gemma3\", process_gemma3)\n",
    "workflow.add_node(\"process_llama2\", process_llama2)\n",
    "workflow.add_node(\"process_llama3\", process_llama3_1)\n",
    "workflow.add_node(\"save_responses\", save_intermediate_results)\n",
    "workflow.add_node(\"evaluate\", evaluate_responses)\n",
    "\n",
    "# 엣지 연결 (기존과 동일)\n",
    "workflow.set_entry_point(\"process_llama2\")\n",
    "workflow.add_edge(\"process_llama2\", \"process_llama3\")\n",
    "workflow.add_edge(\"process_llama3\", \"save_responses\")\n",
    "workflow.add_edge(\"save_responses\", \"evaluate\")\n",
    "workflow.add_edge(\"evaluate\", END)\n",
    "\n",
    "# compile workflow (기존과 동일)\n",
    "app = workflow.compile()\n",
    "\n",
    "# run workflow (기존과 동일)\n",
    "print(\"--- Starting Workflow ---\")\n",
    "final_state = app.invoke(initial_state)\n",
    "print(\"--- Workflow Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- save final results (using timestamp from final_state) ---\n",
    "final_output_dir = \"_output\"\n",
    "os.makedirs(final_output_dir, exist_ok=True)\n",
    "\n",
    "# 3. 최종 상태에서 타임스탬프 가져와서 사용\n",
    "final_timestamp = final_state.get(\"timestamp\", datetime.now().strftime(\"%Y%m%d%H%M%S_final_fallback\"))\n",
    "if not final_timestamp:\n",
    "    print(\"  Warning: Timestamp not found in final state. Generating a new one for fallback.\")\n",
    "    final_timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S_final_fallback\")\n",
    "\n",
    "\n",
    "final_base_name, _ = os.path.splitext(os.path.basename(bench_path)) # 파일명 생성시 bench_path에서 확장자 제거\n",
    "final_filename = f\"{final_timestamp}_{final_base_name}_with_evaluation.json\" # 수정됨\n",
    "final_output_file_path = os.path.join(final_output_dir, final_filename)\n",
    "\n",
    "try:\n",
    "    with open(final_output_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_state, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\nFinal results (with evaluation) successfully saved to {final_output_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving final results: {e}\")\n",
    "    print(f\"Final state type: {type(final_state)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
